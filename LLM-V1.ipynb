{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.device)\n",
    "\n",
    "\"\"\"\n",
    "1. Test different loss functions (ambrose, better weights)\n",
    "2. Test different models (like Temporal CNN, bigger linear model (keeping track of hyperparameters)\n",
    "    https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "3. Implement CAFA-Evaluator for better metrics\n",
    "4. Use more GOs in predictions\n",
    "5. Read Kaggle notebooks online to gain intuition\n",
    "6. Use new data!\n",
    "7. Using description of each GO for making predictions rather than considering them as labels\n",
    "8. Implement winning Kaggle models\n",
    "9. Debug current code\n",
    "10. Use taxonomy (one-hot encoded, embedded)\n",
    "\n",
    "*** Add more information from the Kaggle + Article stuff to the powerpoint\n",
    "*** Create a schema of what model we want to create\n",
    "\"\"\"\n",
    "\n",
    "# esm2_t33_650M_UR50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAIN_DIR = \"data/\"\n",
    "WORK_DIR = \"working/\"\n",
    "DATA_DIR = MAIN_DIR + \"cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"protbert-embeddings-for-cafa5\"\n",
    "ESM2_DIR = MAIN_DIR + \"cafa-5-esm-2-embeddings-numpy\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load a sample submission for Kaggle competition\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define important configurations of the code\n",
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 5000\n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    gamma = 0.7\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "#\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "# \n",
    "# print(f\"GENERATE TARGETS FOR ENTRY IDS ({config.num_labels} MOST COMMON GO TERMS)\")\n",
    "# ids = np.load(f\"{ESM2_DIR}/train_ids.npy\")\n",
    "# labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "# \n",
    "# top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "# labels_names = top_terms[:config.num_labels].index.values\n",
    "# train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "# id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "# \n",
    "# go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "# labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "# \n",
    "# for index, id in tqdm(enumerate(ids)):\n",
    "#     id_gos_list = id_labels[id]\n",
    "#     temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "#     labels_matrix[index, temp] = 1\n",
    "# \n",
    "# labels_list = []\n",
    "# for l in range(labels_matrix.shape[0]):\n",
    "#     labels_list.append(labels_matrix[l, :])\n",
    "# \n",
    "# labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "# labels_df.to_pickle(f\"{ESM2_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "# print(\"GENERATION FINISHED!\")\n",
    "# labels_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IF you already have saved the labels\n",
    "labels_df = pd.read_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}.pkl\")\n",
    "labels_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GO_weight_dataset = pd.read_table(f'{DATA_DIR}/IA.txt', header=None, names=['GO', 'weight'])\n",
    "# GO_weight_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # load GO_weights (IA data) as a tensor to feed into the loss function\n",
    "# GO_weights = []\n",
    "# for each_label in labels_names:\n",
    "#     GO_weights.append(GO_weight_dataset.loc[GO_weight_dataset['GO'] == each_label]['weight'].values[0])\n",
    "# \n",
    "# GO_weights = torch.tensor(GO_weights, dtype=torch.float32)\n",
    "# GO_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"ESM2\" : \"cafa-5-esm-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"ESM2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "      EntryID                                              embed  \\\n0      Q9ZSA8  [-0.092291094, -0.066283956, -0.01226195, 0.04...   \n1      P25353  [0.011624349, -0.030317612, -0.0058019715, 0.0...   \n2  A0A2R8YCW8  [0.02737274, -0.041047025, -0.029205365, 0.028...   \n3      G3V5N8  [0.033766113, -0.07888931, -0.05974137, 0.0456...   \n4  A0A140LFN4  [0.0119482, -0.002107593, -0.084922194, 0.0687...   \n5      B8ZZU6  [0.020136692, -0.13927373, -0.04045331, 0.0626...   \n6      Q01850  [0.026013047, -0.08974387, -0.020240448, 0.132...   \n7      P11076  [0.005006821, -0.03306428, -0.037696116, 0.059...   \n8      Q9VJ64  [0.029283574, -0.010495956, -0.013060905, 0.02...   \n9      Q7YSJ4  [0.02852764, -0.049777817, -0.056415968, 0.141...   \n\n                                         labels_vect  \n0  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n1  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n2  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n3  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n4  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n5  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n8  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n9  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>embed</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q9ZSA8</td>\n      <td>[-0.092291094, -0.066283956, -0.01226195, 0.04...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P25353</td>\n      <td>[0.011624349, -0.030317612, -0.0058019715, 0.0...</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A2R8YCW8</td>\n      <td>[0.02737274, -0.041047025, -0.029205365, 0.028...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>G3V5N8</td>\n      <td>[0.033766113, -0.07888931, -0.05974137, 0.0456...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A140LFN4</td>\n      <td>[0.0119482, -0.002107593, -0.084922194, 0.0687...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>B8ZZU6</td>\n      <td>[0.020136692, -0.13927373, -0.04045331, 0.0626...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Q01850</td>\n      <td>[0.026013047, -0.08974387, -0.020240448, 0.132...</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>P11076</td>\n      <td>[0.005006821, -0.03306428, -0.037696116, 0.059...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Q9VJ64</td>\n      <td>[0.029283574, -0.010495956, -0.013060905, 0.02...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Q7YSJ4</td>\n      <td>[0.02852764, -0.049777817, -0.056415968, 0.141...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to store embeddings of different sources\n",
    "    It could be used to get training or test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"ESM2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "        \n",
    "        # TO BE REMOVED\n",
    "        # self.df = self.df[:1000]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"ESM2\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:02:31.511860400Z",
     "start_time": "2024-02-14T03:02:14.003972900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPONENTS FOR FIRST PROTEIN:  \n",
      "EMBEDDINGS VECTOR: \n",
      "  tensor([-0.0923, -0.0663, -0.0123,  ..., -0.1607,  0.0159,  0.0017]) \n",
      "\n",
      "TARGETS LABELS VECTOR: \n",
      "  tensor([0., 1., 0.,  ..., 0., 0., 0.]) \n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:02:31.533181600Z",
     "start_time": "2024-02-14T03:02:31.514860500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Adjusted MLP model with 6 linear layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1280)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(1280)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(1280, 1800)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1800)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(1800, 2560)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.batchnorm3 = nn.BatchNorm1d(2560)\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.linear4 = torch.nn.Linear(2560, 3200)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.batchnorm4 = nn.BatchNorm1d(3200)\n",
    "        self.dropout4 = nn.Dropout()\n",
    "\n",
    "        self.linear5 = torch.nn.Linear(3200, 4200)\n",
    "        self.activation5 = torch.nn.ReLU()\n",
    "        self.batchnorm5 = nn.BatchNorm1d(4200)\n",
    "        self.dropout5 = nn.Dropout()\n",
    "\n",
    "        self.linear6 = torch.nn.Linear(4200, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.linear5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = self.linear6(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:02:31.583914500Z",
     "start_time": "2024-02-14T03:02:31.535185500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN-1D model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:02:31.628842200Z",
     "start_time": "2024-02-14T03:02:31.551524100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def get_train_val_dataloader(embeddings_source, train_size=0.9):\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset,\n",
    "                                      lengths=[int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:02:31.660845700Z",
     "start_time": "2024-02-14T03:02:31.563090100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, embeddings_source, model_type):\n",
    "    \"\"\"\n",
    "    Custom function to train the baseline model on dataset\n",
    "    :param val_dataloader: dataloader for validation data\n",
    "    :param train_dataloader: dataloader for training data\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    :param model_type: define the type of model\n",
    "    \"\"\"\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    model.to(config.device)\n",
    "\n",
    "    # define configurations of the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=config.gamma)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "\n",
    "    # multilabel prediction task, GO_weights could be used as weight in the loss function and f1score\n",
    "    # GO_weights.to(config.device)\n",
    "    MultiLabelLoss = torch.nn.BCEWithLogitsLoss()\n",
    "    # f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_f1score_history, val_f1score_history = [], []\n",
    "    train_accuracy_history, val_accuracy_history = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "\n",
    "        ## TRAIN PHASE :\n",
    "        model.train()\n",
    "        losses, scores, accuracy = [], [], []\n",
    "\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            preds = (sigmoid(preds).detach().cpu().numpy() > 0.5).astype(int)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            score = f1_score(targets, preds, average='micro')\n",
    "            acc = np.mean([accuracy_score(targets[i], preds[i]) for i in range(len(targets))])\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score)\n",
    "            accuracy.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        avg_accuracy = np.mean(accuracy)\n",
    "        print(\"Running Average TRAIN Loss: \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score: \", avg_score)\n",
    "        print(\"Running Average TRAIN Accuracy: \", avg_accuracy)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "        train_accuracy_history.append(avg_accuracy)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        model.eval()\n",
    "        losses, scores, accuracy = [], [], []\n",
    "\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            preds = (sigmoid(preds).detach().cpu().numpy() > 0.5).astype(int)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            score = f1_score(targets, preds, average='micro')\n",
    "            acc = np.mean([accuracy_score(targets[i], preds[i]) for i in range(len(targets))])\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score)\n",
    "            accuracy.append(acc)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        avg_accuracy = np.mean(accuracy)\n",
    "        print(\"Running Average VAL Loss: \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score: \", avg_score)\n",
    "        print(\"Running Average VAL Accuracy: \", avg_accuracy)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "        val_accuracy_history.append(avg_accuracy)\n",
    "\n",
    "        scheduler.step(), print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"TRAINING FINISHED _____\")\n",
    "    print(f\"FINAL TRAINING F1SCORE: {train_f1score_history[-1]},  ACCURACY: {train_accuracy_history[-1]}\")\n",
    "    print(f\"FINAL VALIDATION F1SCORE: {val_f1score_history[-1]},  ACCURACY: {val_accuracy_history[-1]}\")\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "    accuracy_history = {\"train\" : train_accuracy_history, \"val\" : val_accuracy_history}\n",
    "\n",
    "    return model, losses_history, scores_history, accuracy_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:02:31.707843500Z",
     "start_time": "2024-02-14T03:02:31.586913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [06:46<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss:  0.02911109179556072\n",
      "Running Average TRAIN F1-Score:  0.21446737255331683\n",
      "Running Average TRAIN Accuracy:  0.9926801793444652\n",
      "Running Average VAL Loss:  0.025612865208781192\n",
      "Running Average VAL F1-Score:  0.23730500309144847\n",
      "Running Average VAL Accuracy:  0.9933339392397584\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [07:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss:  0.025668241494855325\n",
      "Running Average TRAIN F1-Score:  0.24569909302627843\n",
      "Running Average TRAIN Accuracy:  0.9932891609134913\n",
      "Running Average VAL Loss:  0.024395958221118366\n",
      "Running Average VAL F1-Score:  0.2838918328775604\n",
      "Running Average VAL Accuracy:  0.9934048368566176\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [06:42<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss:  0.024816529940698412\n",
      "Running Average TRAIN F1-Score:  0.2582358126355202\n",
      "Running Average TRAIN Accuracy:  0.9933537861691879\n",
      "Running Average VAL Loss:  0.023908699230690087\n",
      "Running Average VAL F1-Score:  0.2722161498017742\n",
      "Running Average VAL Accuracy:  0.9934272748161764\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 847/1001 [05:39<01:01,  2.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[77], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m train_dataloader, val_dataloader \u001B[38;5;241m=\u001B[39m get_train_val_dataloader(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mESM2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m esm2_model, esm2_losses, esm2_scores, esm2_accuracy \u001B[38;5;241m=\u001B[39m train_model(train_dataloader,\n\u001B[0;32m      3\u001B[0m                                                                   val_dataloader,\n\u001B[0;32m      4\u001B[0m                                                                   embeddings_source\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mESM2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      5\u001B[0m                                                                   model_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[76], line 48\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(train_dataloader, val_dataloader, embeddings_source, model_type)\u001B[0m\n\u001B[0;32m     45\u001B[0m preds \u001B[38;5;241m=\u001B[39m (sigmoid(preds)\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m     46\u001B[0m targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m---> 48\u001B[0m score \u001B[38;5;241m=\u001B[39m f1_score(targets, preds, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmicro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     49\u001B[0m acc \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean([accuracy_score(targets[i], preds[i]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(targets))])\n\u001B[0;32m     50\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    209\u001B[0m         )\n\u001B[0;32m    210\u001B[0m     ):\n\u001B[1;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    221\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1238\u001B[0m, in \u001B[0;36mf1_score\u001B[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   1071\u001B[0m     {\n\u001B[0;32m   1072\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1096\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1097\u001B[0m ):\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \n\u001B[0;32m   1100\u001B[0m \u001B[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1236\u001B[0m \u001B[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001B[39;00m\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1238\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fbeta_score(\n\u001B[0;32m   1239\u001B[0m         y_true,\n\u001B[0;32m   1240\u001B[0m         y_pred,\n\u001B[0;32m   1241\u001B[0m         beta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1242\u001B[0m         labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[0;32m   1243\u001B[0m         pos_label\u001B[38;5;241m=\u001B[39mpos_label,\n\u001B[0;32m   1244\u001B[0m         average\u001B[38;5;241m=\u001B[39maverage,\n\u001B[0;32m   1245\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m   1246\u001B[0m         zero_division\u001B[38;5;241m=\u001B[39mzero_division,\n\u001B[0;32m   1247\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    182\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    186\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1411\u001B[0m, in \u001B[0;36mfbeta_score\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1250\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   1251\u001B[0m     {\n\u001B[0;32m   1252\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1278\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1279\u001B[0m ):\n\u001B[0;32m   1280\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the F-beta score.\u001B[39;00m\n\u001B[0;32m   1281\u001B[0m \n\u001B[0;32m   1282\u001B[0m \u001B[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1408\u001B[0m \u001B[38;5;124;03m    0.38...\u001B[39;00m\n\u001B[0;32m   1409\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1411\u001B[0m     _, _, f, _ \u001B[38;5;241m=\u001B[39m precision_recall_fscore_support(\n\u001B[0;32m   1412\u001B[0m         y_true,\n\u001B[0;32m   1413\u001B[0m         y_pred,\n\u001B[0;32m   1414\u001B[0m         beta\u001B[38;5;241m=\u001B[39mbeta,\n\u001B[0;32m   1415\u001B[0m         labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[0;32m   1416\u001B[0m         pos_label\u001B[38;5;241m=\u001B[39mpos_label,\n\u001B[0;32m   1417\u001B[0m         average\u001B[38;5;241m=\u001B[39maverage,\n\u001B[0;32m   1418\u001B[0m         warn_for\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf-score\u001B[39m\u001B[38;5;124m\"\u001B[39m,),\n\u001B[0;32m   1419\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m   1420\u001B[0m         zero_division\u001B[38;5;241m=\u001B[39mzero_division,\n\u001B[0;32m   1421\u001B[0m     )\n\u001B[0;32m   1422\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    182\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    186\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1725\u001B[0m, in \u001B[0;36mprecision_recall_fscore_support\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1723\u001B[0m \u001B[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001B[39;00m\n\u001B[0;32m   1724\u001B[0m samplewise \u001B[38;5;241m=\u001B[39m average \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1725\u001B[0m MCM \u001B[38;5;241m=\u001B[39m multilabel_confusion_matrix(\n\u001B[0;32m   1726\u001B[0m     y_true,\n\u001B[0;32m   1727\u001B[0m     y_pred,\n\u001B[0;32m   1728\u001B[0m     sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m   1729\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[0;32m   1730\u001B[0m     samplewise\u001B[38;5;241m=\u001B[39msamplewise,\n\u001B[0;32m   1731\u001B[0m )\n\u001B[0;32m   1732\u001B[0m tp_sum \u001B[38;5;241m=\u001B[39m MCM[:, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   1733\u001B[0m pred_sum \u001B[38;5;241m=\u001B[39m tp_sum \u001B[38;5;241m+\u001B[39m MCM[:, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    182\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    186\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:505\u001B[0m, in \u001B[0;36mmultilabel_confusion_matrix\u001B[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001B[0m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m    396\u001B[0m     {\n\u001B[0;32m    397\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    406\u001B[0m     y_true, y_pred, \u001B[38;5;241m*\u001B[39m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, samplewise\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    407\u001B[0m ):\n\u001B[0;32m    408\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute a confusion matrix for each class or sample.\u001B[39;00m\n\u001B[0;32m    409\u001B[0m \n\u001B[0;32m    410\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 0.21\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    503\u001B[0m \u001B[38;5;124;03m            [1, 2]]])\u001B[39;00m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 505\u001B[0m     y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m _check_targets(y_true, y_pred)\n\u001B[0;32m    506\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    507\u001B[0m         sample_weight \u001B[38;5;241m=\u001B[39m column_or_1d(sample_weight)\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;124;03my_pred : array or indicator matrix\u001B[39;00m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     84\u001B[0m check_consistent_length(y_true, y_pred)\n\u001B[1;32m---> 85\u001B[0m type_true \u001B[38;5;241m=\u001B[39m type_of_target(y_true, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     86\u001B[0m type_pred \u001B[38;5;241m=\u001B[39m type_of_target(y_pred, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     88\u001B[0m y_type \u001B[38;5;241m=\u001B[39m {type_true, type_pred}\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:310\u001B[0m, in \u001B[0;36mtype_of_target\u001B[1;34m(y, input_name)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse_pandas:\n\u001B[0;32m    308\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my cannot be class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparseSeries\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparseArray\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_multilabel(y):\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel-indicator\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001B[39;00m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001B[39;00m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;66;03m# value error (NumPy >= 1.24).\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:188\u001B[0m, in \u001B[0;36mis_multilabel\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    183\u001B[0m         \u001B[38;5;28mlen\u001B[39m(y\u001B[38;5;241m.\u001B[39mdata) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    184\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m (labels\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (labels\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01min\u001B[39;00m labels))\n\u001B[0;32m    185\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m (y\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbiu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m _is_integral_float(labels))  \u001B[38;5;66;03m# bool, int, uint\u001B[39;00m\n\u001B[0;32m    186\u001B[0m     )\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 188\u001B[0m     labels \u001B[38;5;241m=\u001B[39m xp\u001B[38;5;241m.\u001B[39munique_values(y)\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(labels) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[0;32m    191\u001B[0m         y\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbiu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m _is_integral_float(labels)  \u001B[38;5;66;03m# bool, int, uint\u001B[39;00m\n\u001B[0;32m    192\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:262\u001B[0m, in \u001B[0;36m_NumPyAPIWrapper.unique_values\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munique_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39munique(x)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36munique\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\arraysetops.py:274\u001B[0m, in \u001B[0;36munique\u001B[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[0;32m    272\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 274\u001B[0m     ret \u001B[38;5;241m=\u001B[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001B[0;32m    275\u001B[0m                     equal_nan\u001B[38;5;241m=\u001B[39mequal_nan)\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\arraysetops.py:336\u001B[0m, in \u001B[0;36m_unique1d\u001B[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[0;32m    334\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 336\u001B[0m     ar\u001B[38;5;241m.\u001B[39msort()\n\u001B[0;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[0;32m    338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_train_val_dataloader(\"ESM2\")\n",
    "esm2_model, esm2_losses, esm2_scores, esm2_accuracy = train_model(train_dataloader,\n",
    "                                                                  val_dataloader,\n",
    "                                                                  embeddings_source=\"ESM2\",\n",
    "                                                                  model_type=\"linear\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:31:39.314883900Z",
     "start_time": "2024-02-14T03:02:31.597844300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = next(iter(val_dataloader))[0][1].unsqueeze(0)\n",
    "label = next(iter(val_dataloader))[1][1].unsqueeze(0)\n",
    "print(f\"Sample:\\n{sample}\\n\\nLabel:\\n{label}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model.to('cpu')\n",
    "esm2_model.eval()\n",
    "output = esm2_model(sample)\n",
    "output = torch.round(sigmoid(output))\n",
    "output = output.detach().cpu().numpy()\n",
    "accuracy_score(label[0], output[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t33_650M_UR50D')\n",
    "embedding_model = EsmModel.from_pretrained('facebook/esm2_t33_650M_UR50D', add_cross_attention=False, is_decoder=False)\n",
    "embedding_model.eval()\n",
    "embedding_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ids = tokenizer(['MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN', 'QQQQQ'], add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])\n",
    "output = embedding_model(input_ids=input_ids, attention_mask=attention_mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = esm2_model(output.last_hidden_state[0].to(device))\n",
    "cls = esm2_model(output.last_hidden_state[0][0].to(device))\n",
    "torch.nn.functional.sigmoid(cls) > 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(embeddings_source, data_source):\n",
    "    \"\"\"\n",
    "    Custom function to make inference using the model\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    \"\"\"\n",
    "\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "\n",
    "    if data_source == \"val\":\n",
    "        predict_dataloader = val_dataloader\n",
    "\n",
    "    if data_source == 'test':\n",
    "        predict_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"ESM2\":\n",
    "        model = esm2_model\n",
    "\n",
    "    # Set model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(predict_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_df = predict(\"T5\", \"val\")\n",
    "submission_df.to_tsv(\"/working/predictions_val.tsv\")\n",
    "submission_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### SCRIPT TO EVALUATE PREDICTIONS USING CAFA EVALUATOR ###\n",
    "\n",
    "import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval\n",
    "\n",
    "cafa_eval(f\"{DATA_DIR}/Train/go-basic.obo\", submission_df, f\"{DATA_DIR}/Train/train_terms.tsv\", ia=f\"{DATA_DIR}/IA.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### IN PROGRESS - SCRIPT TO TRAIN THE MODEL USING PyTorchLightning ###\n",
    "\n",
    "class Linear_Lightning(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    In progress, used to train the MLP model on multiple GPUs using PyTorchLightning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, train_size, **hparams):\n",
    "        super(Linear_Lightning, self).__init__()\n",
    "\n",
    "        self.model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "        train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "        self.train_set, self.val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n",
    "        self.accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        logs = {\"train_loss\" : loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss= self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        return {\"val_loss\": loss, \"f1_score\": f1_score, \"accuracy_score\": acc_score}\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in ouputs]).mean()\n",
    "        logs = {\"val_loss\" : avg_loss}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = torch.utils.data.DataLoader(self.val_set, batch_size=config.batch_size, shuffle=False,)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.n_epochs,\n",
    "    limit_train_batches=5000,\n",
    "    logger=logger)\n",
    "\n",
    "\n",
    "model = Linear_Lightning(\n",
    "    input_dim=embeds_dim[embeddings_source],\n",
    "    num_classes=config.num_labels,\n",
    "    train_size=0.8)\n",
    "\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
