{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.device)\n",
    "\n",
    "\"\"\"\n",
    "1. Test different loss functions (ambrose, better weights)\n",
    "2. Test different models (like Temporal CNN, bigger linear model (keeping track of hyperparameters)\n",
    "    https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "3. Implement CAFA-Evaluator for better metrics\n",
    "4. Use more GOs in predictions\n",
    "5. Read Kaggle notebooks online to gain intuition\n",
    "6. Use new data!\n",
    "7. Using description of each GO for making predictions rather than considering them as labels\n",
    "8. Implement winning Kaggle models\n",
    "9. Debug current code\n",
    "10. Use taxonomy (one-hot encoded, embedded)\n",
    "\n",
    "*** Add more information from the Kaggle + Article stuff to the powerpoint\n",
    "*** Create a schema of what model we want to create\n",
    "\"\"\"\n",
    "\n",
    "# esm2_t33_650M_UR50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAIN_DIR = \"data\"\n",
    "WORK_DIR = \"working\"\n",
    "DATA_DIR = MAIN_DIR + \"/cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"/protbert-embeddings-for-cafa5\"\n",
    "ESM2_DIR = MAIN_DIR + \"/cafa-5-esm-2-embeddings-numpy\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load a sample submission for Kaggle competition\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define important configurations of the code\n",
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 5000\n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    val_batch_size = 256\n",
    "    lr = 0.001\n",
    "    gamma = 0.7\n",
    "    dataset_size = 142246\n",
    "\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "#\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "\n",
    "print(f\"GENERATE TARGETS FOR ENTRY IDS ({config.num_labels} MOST COMMON GO TERMS)\")\n",
    "ids = np.load(f\"{ESM2_DIR}/train_ids.npy\")\n",
    "labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "\n",
    "top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "labels_names = top_terms[:config.num_labels].index.values\n",
    "train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "for index, id in tqdm(enumerate(ids)):\n",
    "    id_gos_list = id_labels[id]\n",
    "    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "    labels_matrix[index, temp] = 1\n",
    "\n",
    "labels_list = []\n",
    "for l in range(labels_matrix.shape[0]):\n",
    "    labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "labels_df.to_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}.pkl\")\n",
    "print(\"GENERATION FINISHED!\")\n",
    "labels_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Save and labels as a sparse matrix\n",
    "# labels_list_sparse = csr_matrix(labels_list)\n",
    "# labels_list_sparse.to_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}_sparse.pkl\")\n",
    "# labels_list_sparse = pd.read_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}_sparse.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load GO_weights (IA data) as a tensor to feed into the loss function\n",
    "\n",
    "GO_weight_dataset = pd.read_table(f'{DATA_DIR}/IA.txt', header=None, names=['GO', 'weight'])\n",
    "GO_weights = []\n",
    "for each_label in labels_names:\n",
    "    GO_weights.append(GO_weight_dataset.loc[GO_weight_dataset['GO'] == each_label]['weight'].values[0])\n",
    "\n",
    "GO_weights = torch.tensor(GO_weights, dtype=torch.float32)\n",
    "torch.save(GO_weights, f\"{ESM2_DIR}/go_weights_{config.num_labels}.pt\")\n",
    "GO_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IF you already have saved the labels and go_weights\n",
    "labels_df = pd.read_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}.pkl\")\n",
    "GO_weights = torch.load(f\"{ESM2_DIR}/go_weights_{config.num_labels}.pt\")\n",
    "print(f'Labels shape: {labels_df.shape}, GO Weights shape: {GO_weights.shape}')\n",
    "labels_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"ESM2\" : \"cafa-5-esm-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"ESM2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to store embeddings of different sources\n",
    "    It could be used to get training or test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"ESM2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{MAIN_DIR}/{embeds_map[embeddings_source]}/train_targets_top{config.num_labels}.pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"ESM2\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Adjusted MLP model with 6 linear layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1280)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(1280)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(1280, 1800)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1800)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(1800, 2560)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.batchnorm3 = nn.BatchNorm1d(2560)\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.linear4 = torch.nn.Linear(2560, 3200)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.batchnorm4 = nn.BatchNorm1d(3200)\n",
    "        self.dropout4 = nn.Dropout()\n",
    "\n",
    "        self.linear5 = torch.nn.Linear(3200, 4200)\n",
    "        self.activation5 = torch.nn.ReLU()\n",
    "        self.batchnorm5 = nn.BatchNorm1d(4200)\n",
    "        self.dropout5 = nn.Dropout()\n",
    "\n",
    "        self.linear6 = torch.nn.Linear(4200, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.linear5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = self.linear6(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN-1D model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep Residual Network module with step by step predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, step_dim, hidden_dim=1024):\n",
    "        super(ResidualNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # First Block\n",
    "        self.forward_linear1 = torch.nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear1 = torch.nn.Linear(self.hidden_dim, step_dim[0])\n",
    "\n",
    "\n",
    "        # Second Block\n",
    "        self.concat1_shape = self.hidden_dim + step_dim[0]\n",
    "        self.forward_linear2 = torch.nn.Linear(self.concat1_shape, self.hidden_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear2 = torch.nn.Linear(self.hidden_dim, step_dim[1])\n",
    "\n",
    "\n",
    "        # Third Block\n",
    "        self.concat2_shape = self.hidden_dim + step_dim[0] + step_dim[1]\n",
    "        self.forward_linear3 = torch.nn.Linear(self.concat2_shape, self.hidden_dim)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear3 = torch.nn.Linear(self.hidden_dim, step_dim[2])\n",
    "\n",
    "\n",
    "        # Fourth Block\n",
    "        self.concat3_shape = self.hidden_dim + step_dim[0] + step_dim[1] + step_dim[2]\n",
    "        self.forward_linear4 = torch.nn.Linear(self.concat3_shape, self.hidden_dim)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear4 = torch.nn.Linear(self.hidden_dim, step_dim[3])\n",
    "\n",
    "\n",
    "    def forward(self, input_embed):\n",
    "        input_embed = self.forward_linear1(input_embed)\n",
    "        input_embed = self.batchnorm1(input_embed)\n",
    "        input_embed = self.activation1(input_embed)\n",
    "        input_embed = self.dropout1(input_embed)\n",
    "\n",
    "        y1 = self.pred_linear1(input_embed)\n",
    "\n",
    "        x = torch.cat([input_embed, y1], dim=1)\n",
    "        x = self.forward_linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        y2 = self.pred_linear2(x)\n",
    "\n",
    "        x = torch.cat([input_embed + x, y1, y2], dim=1)\n",
    "        x = self.forward_linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        y3 = self.pred_linear3(x)\n",
    "\n",
    "        x = torch.cat([input_embed + x, y1, y2, y3], dim=1)\n",
    "        x = self.forward_linear4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        y4 = self.pred_linear4(x)\n",
    "\n",
    "        y = torch.cat([y1, y2, y3, y4], dim=1)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_val_dataloader(embeddings_source, train_size=0.9):\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset,\n",
    "                                      lengths=[int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=config.val_batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, embeddings_source, model_type):\n",
    "    \"\"\"\n",
    "    Custom function to train the baseline model on dataset\n",
    "    :param val_dataloader: dataloader for validation data\n",
    "    :param train_dataloader: dataloader for training data\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    :param model_type: define the type of model\n",
    "    \"\"\"\n",
    "    if model_type == 'residual':\n",
    "        model = ResidualNetwork(input_dim=embeds_dim[embeddings_source], step_dim=[1500, 1250, 1250, 1000])\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    print(f'Model:\\n{model}\\n\\n')\n",
    "    model.to(config.device)\n",
    "\n",
    "    # define configurations of the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=config.gamma)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "\n",
    "    # multilabel prediction task, GO_weights could be used as weight in the loss function and f1score\n",
    "    GO_weights.to(config.device)\n",
    "    pos_weight = torch.Tensor([25]).to(config.device)\n",
    "    MultiLabelLoss = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)   # weight=GO_weights+1\n",
    "    # f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_f1score_history, val_f1score_history = [], []\n",
    "    train_accuracy_history, val_accuracy_history = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "\n",
    "        ## TRAIN PHASE :\n",
    "        model.train()\n",
    "        losses, scores, accuracy = [], [], []\n",
    "\n",
    "\n",
    "        for i, (embed, targets) in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            # if i == 500:\n",
    "            #     break\n",
    "\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            probs = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(target=targets, input=probs)\n",
    "            preds = (sigmoid(probs).detach().cpu().numpy() > 0.5).astype(int)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            # score = f1_score(targets, preds, average='macro') #, sample_weight=GO_weights\n",
    "            acc = np.mean([balanced_accuracy_score(targets[i], preds[i], sample_weight=GO_weights) for i in range(len(targets))])\n",
    "            losses.append(loss.item())\n",
    "            # scores.append(score)\n",
    "            accuracy.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # return targets, preds\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        # avg_score = np.mean(scores)\n",
    "        avg_accuracy = np.mean(accuracy)\n",
    "        print(\"Running Average TRAIN Loss: \", avg_loss)\n",
    "        # print(\"Running Average TRAIN F1-Score: \", avg_score)\n",
    "        print(\"Running Average TRAIN Accuracy: \", avg_accuracy)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        # train_f1score_history.append(avg_score)\n",
    "        train_accuracy_history.append(avg_accuracy)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        model.eval()\n",
    "        losses, accuracy, val_preds, val_targets, val_probs = [], [], [], [], []\n",
    "\n",
    "        for i, (embed, targets) in enumerate(val_dataloader):\n",
    "\n",
    "            # if i == 50:\n",
    "            #     break\n",
    "\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            probs = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(target=targets, input=probs)\n",
    "\n",
    "            preds = (sigmoid(probs).detach().cpu().numpy() > 0.5).astype(int)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            probs = probs.detach().cpu().numpy()\n",
    "\n",
    "            val_preds.append(preds)\n",
    "            val_targets.append(targets)\n",
    "            val_probs.append(probs)\n",
    "            acc = np.mean([balanced_accuracy_score(targets[i], preds[i], sample_weight=GO_weights) for i in range(len(targets))])\n",
    "            losses.append(loss.item())\n",
    "            accuracy.append(acc)\n",
    "\n",
    "        val_preds = np.vstack(val_preds)\n",
    "        val_targets = np.vstack(val_targets)\n",
    "        val_probs = np.vstack(val_probs)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_accuracy = np.mean(accuracy)\n",
    "\n",
    "        avg_precision = precision_score(val_targets, val_preds, average=None)\n",
    "        avg_recall = recall_score(val_targets, val_preds, average=None)\n",
    "        print(f'precision:\\n{np.mean(avg_precision)}\\n\\nrecall:\\n{np.mean(avg_recall)}')\n",
    "\n",
    "        avg_f1_score = f1_score(val_targets, val_preds, average=None)\n",
    "        avg_f1_score = np.dot(avg_f1_score, np.ones_like(GO_weights)) / sum(np.ones_like(GO_weights))\n",
    "\n",
    "        print(\"Running Average VAL Loss: \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score: \", avg_f1_score)\n",
    "        print(\"Running Average VAL Accuracy: \", avg_accuracy)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_f1_score)\n",
    "        val_accuracy_history.append(avg_accuracy)\n",
    "\n",
    "        scheduler.step(), print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"TRAINING FINISHED _____\")\n",
    "    print(f\"FINAL TRAINING F1SCORE: {train_f1score_history[-1]},  ACCURACY: {train_accuracy_history[-1]}\")\n",
    "    print(f\"FINAL VALIDATION F1SCORE: {val_f1score_history[-1]},  ACCURACY: {val_accuracy_history[-1]}\")\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "    accuracy_history = {\"train\" : train_accuracy_history, \"val\" : val_accuracy_history}\n",
    "\n",
    "    return model, losses_history, scores_history, accuracy_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_train_val_dataloader(\"ESM2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds, targets = train_model(train_dataloader,\n",
    "                             val_dataloader,\n",
    "                             embeddings_source=\"ESM2\",\n",
    "                             model_type=\"residual\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_pred=preds[0], y_true=targets[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score(y_true=targets, y_pred=preds, average=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preds\n",
    "sum(targets[30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# targets\n",
    "sum(preds[4000])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model, esm2_losses, esm2_scores, esm2_accuracy = train_model(train_dataloader,\n",
    "                                                                  val_dataloader,\n",
    "                                                                  embeddings_source=\"ESM2\",\n",
    "                                                                  model_type=\"residual\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model.to('cpu')\n",
    "esm2_model.eval()\n",
    "output = esm2_model(sample)\n",
    "output = torch.round(sigmoid(output))\n",
    "output = output.detach().cpu().numpy()\n",
    "accuracy_score(label[0], output[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t33_650M_UR50D')\n",
    "embedding_model = EsmModel.from_pretrained('facebook/esm2_t33_650M_UR50D', add_cross_attention=False, is_decoder=False)\n",
    "embedding_model.eval()\n",
    "embedding_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ids = tokenizer(['MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN', 'QQQQQ'], add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])\n",
    "output = embedding_model(input_ids=input_ids, attention_mask=attention_mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = esm2_model(output.last_hidden_state[0].to(device))\n",
    "cls = esm2_model(output.last_hidden_state[0][0].to(device))\n",
    "torch.nn.functional.sigmoid(cls) > 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(embeddings_source, data_source):\n",
    "    \"\"\"\n",
    "    Custom function to make inference using the model\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    \"\"\"\n",
    "\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "\n",
    "    if data_source == \"val\":\n",
    "        predict_dataloader = val_dataloader\n",
    "\n",
    "    if data_source == 'test':\n",
    "        predict_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"ESM2\":\n",
    "        model = esm2_model\n",
    "\n",
    "    # Set model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(predict_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_df = predict(\"T5\", \"val\")\n",
    "submission_df.to_tsv(\"/working/predictions_val.tsv\")\n",
    "submission_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### SCRIPT TO EVALUATE PREDICTIONS USING CAFA EVALUATOR ###\n",
    "\n",
    "import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval\n",
    "\n",
    "cafa_eval(f\"{DATA_DIR}/Train/go-basic.obo\", submission_df, f\"{DATA_DIR}/Train/train_terms.tsv\", ia=f\"{DATA_DIR}/IA.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### IN PROGRESS - SCRIPT TO TRAIN THE MODEL USING PyTorchLightning ###\n",
    "\n",
    "class Linear_Lightning(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    In progress, used to train the MLP model on multiple GPUs using PyTorchLightning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, train_size, **hparams):\n",
    "        super(Linear_Lightning, self).__init__()\n",
    "\n",
    "        self.model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "        train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "        self.train_set, self.val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n",
    "        self.accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        logs = {\"train_loss\" : loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss= self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        return {\"val_loss\": loss, \"f1_score\": f1_score, \"accuracy_score\": acc_score}\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in ouputs]).mean()\n",
    "        logs = {\"val_loss\" : avg_loss}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = torch.utils.data.DataLoader(self.val_set, batch_size=config.batch_size, shuffle=False,)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.n_epochs,\n",
    "    limit_train_batches=5000,\n",
    "    logger=logger)\n",
    "\n",
    "\n",
    "model = Linear_Lightning(\n",
    "    input_dim=embeds_dim[embeddings_source],\n",
    "    num_classes=config.num_labels,\n",
    "    train_size=0.8)\n",
    "\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
