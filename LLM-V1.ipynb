{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.device)\n",
    "\n",
    "\"\"\"\n",
    "1. Test different loss functions (ambrose, better weights)\n",
    "2. Test different models (like Temporal CNN, bigger linear model (keeping track of hyperparameters)\n",
    "    https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "3. Implement CAFA-Evaluator for better metrics\n",
    "4. Use more GOs in predictions\n",
    "5. Read Kaggle notebooks online to gain intuition\n",
    "6. Use new data!\n",
    "7. Using description of each GO for making predictions rather than considering them as labels\n",
    "8. Implement winning Kaggle models\n",
    "\n",
    "*** Add more information from the Kaggle + Article stuff to the powerpoint\n",
    "*** Create a schema of what model we want to create\n",
    "\"\"\"\n",
    "\n",
    "# esm2_t33_650M_UR50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAIN_DIR = \"data/\"\n",
    "WORK_DIR = \"working/\"\n",
    "DATA_DIR = MAIN_DIR + \"cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"protbert-embeddings-for-cafa5\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load a sample submission for Kaggle competition\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define important configurations of the code\n",
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 5000\n",
    "    n_epochs = 20\n",
    "    batch_size = 128\n",
    "    lr = 0.005\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "#\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142246it [11:21, 208.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION FINISHED!\n"
     ]
    },
    {
     "data": {
      "text/plain": "      EntryID                                        labels_vect\n0      P20536  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...\n1      O73864  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...\n2      O95231  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...\n3  A0A0B4J1F4  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...\n4      P54366  [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P20536</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O73864</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O95231</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0B4J1F4</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P54366</td>\n      <td>[1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "\n",
    "print(f\"GENERATE TARGETS FOR ENTRY IDS ({config.num_labels} MOST COMMON GO TERMS)\")\n",
    "ids = np.load(f\"{PROTBERT_DIR}/train_ids.npy\")\n",
    "labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "\n",
    "top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "labels_names = top_terms[:config.num_labels].index.values\n",
    "train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "for index, id in tqdm(enumerate(ids)):\n",
    "    id_gos_list = id_labels[id]\n",
    "    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "    labels_matrix[index, temp] = 1\n",
    "\n",
    "labels_list = []\n",
    "for l in range(labels_matrix.shape[0]):\n",
    "    labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "labels_df.to_pickle(f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "print(\"GENERATION FINISHED!\")\n",
    "labels_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GO_weight_dataset = pd.read_table(f'{DATA_DIR}/IA.txt', header=None, names=['GO', 'weight'])\n",
    "GO_weight_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load GO_weights (IA data) as a tensor to feed into the loss function\n",
    "\n",
    "GO_weights = []\n",
    "for each_label in labels_names:\n",
    "    GO_weights.append(GO_weight_dataset.loc[GO_weight_dataset['GO'] == each_label]['weight'].values[0])\n",
    "\n",
    "GO_weights = torch.tensor(GO_weights, dtype=torch.float32)\n",
    "GO_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"ESM2\" : \"cafa-5-esm-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"ESM2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "      EntryID                                              embed  \\\n0      Q9ZSA8  [-0.092291094, -0.066283956, -0.01226195, 0.04...   \n1      P25353  [0.011624349, -0.030317612, -0.0058019715, 0.0...   \n2  A0A2R8YCW8  [0.02737274, -0.041047025, -0.029205365, 0.028...   \n3      G3V5N8  [0.033766113, -0.07888931, -0.05974137, 0.0456...   \n4  A0A140LFN4  [0.0119482, -0.002107593, -0.084922194, 0.0687...   \n5      B8ZZU6  [0.020136692, -0.13927373, -0.04045331, 0.0626...   \n6      Q01850  [0.026013047, -0.08974387, -0.020240448, 0.132...   \n7      P11076  [0.005006821, -0.03306428, -0.037696116, 0.059...   \n8      Q9VJ64  [0.029283574, -0.010495956, -0.013060905, 0.02...   \n9      Q7YSJ4  [0.02852764, -0.049777817, -0.056415968, 0.141...   \n\n                                         labels_vect  \n0  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n1  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n2  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n3  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n4  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n5  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n8  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n9  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>embed</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q9ZSA8</td>\n      <td>[-0.092291094, -0.066283956, -0.01226195, 0.04...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P25353</td>\n      <td>[0.011624349, -0.030317612, -0.0058019715, 0.0...</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A2R8YCW8</td>\n      <td>[0.02737274, -0.041047025, -0.029205365, 0.028...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>G3V5N8</td>\n      <td>[0.033766113, -0.07888931, -0.05974137, 0.0456...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A140LFN4</td>\n      <td>[0.0119482, -0.002107593, -0.084922194, 0.0687...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>B8ZZU6</td>\n      <td>[0.020136692, -0.13927373, -0.04045331, 0.0626...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Q01850</td>\n      <td>[0.026013047, -0.08974387, -0.020240448, 0.132...</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>P11076</td>\n      <td>[0.005006821, -0.03306428, -0.037696116, 0.059...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Q9VJ64</td>\n      <td>[0.029283574, -0.010495956, -0.013060905, 0.02...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Q7YSJ4</td>\n      <td>[0.02852764, -0.049777817, -0.056415968, 0.141...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to store embeddings of different sources\n",
    "    It could be used to get training or test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"ESM2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\\\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"ESM2\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPONENTS FOR FIRST PROTEIN:  \n",
      "EMBEDDINGS VECTOR: \n",
      "  tensor([-0.0923, -0.0663, -0.0123,  ..., -0.1607,  0.0159,  0.0017]) \n",
      "\n",
      "TARGETS LABELS VECTOR: \n",
      "  tensor([0., 1., 0.,  ..., 0., 0., 0.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline MLP model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, input_dim)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1000)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(1000, 800)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(800, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN-1D model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_val_dataloader(embeddings_source, train_size=0.9):\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset,\n",
    "                                      lengths=[int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, embeddings_source, model_type):\n",
    "    \"\"\"\n",
    "    Custom function to train the baseline model on dataset\n",
    "    :param val_dataloader: dataloader for validation data\n",
    "    :param train_dataloader: dataloader for training data\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    :param model_type: define the type of model\n",
    "    \"\"\"\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    model.to(config.device)\n",
    "\n",
    "    # define configurations of the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "\n",
    "    # multilabel prediction task, GO_weights could be used as weight in the loss function\n",
    "    # GO_weights.to(config.device)\n",
    "    MultiLabelLoss = torch.nn.BCEWithLogitsLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_f1score_history, val_f1score_history = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "\n",
    "        ## TRAIN PHASE :\n",
    "        model.train()\n",
    "        losses, scores = [], []\n",
    "\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            score = f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        model.eval()\n",
    "        losses, scores = [], []\n",
    "\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device),\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            score = f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "        avg_loss, avg_score = np.mean(losses), np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "\n",
    "        scheduler.step(avg_loss), print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"TRAINING FINISHED _____\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "\n",
    "    return model, losses_history, scores_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:45<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.02852203739392055\n",
      "Running Average TRAIN F1-Score :  0.0038792571760239555\n",
      "Running Average VAL Loss :  0.02545470609662256\n",
      "Running Average VAL F1-Score :  0.006463627511818361\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:43<00:00, 23.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.024512005975464365\n",
      "Running Average TRAIN F1-Score :  0.009580776889480048\n",
      "Running Average VAL Loss :  0.02416943772030728\n",
      "Running Average VAL F1-Score :  0.011527426008667265\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:42<00:00, 23.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.02358632677836673\n",
      "Running Average TRAIN F1-Score :  0.013444428116531595\n",
      "Running Average VAL Loss :  0.023724983612607633\n",
      "Running Average VAL F1-Score :  0.0138590978978235\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:42<00:00, 23.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.022997279291401256\n",
      "Running Average TRAIN F1-Score :  0.01660784735827224\n",
      "Running Average VAL Loss :  0.023309157363006046\n",
      "Running Average VAL F1-Score :  0.01742265691947458\n",
      "\n",
      "\n",
      "EPOCH  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:43<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.022542971431033\n",
      "Running Average TRAIN F1-Score :  0.020041953243843683\n",
      "Running Average VAL Loss :  0.023211120694343532\n",
      "Running Average VAL F1-Score :  0.020264610041132464\n",
      "\n",
      "\n",
      "EPOCH  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:42<00:00, 23.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.022229966059044287\n",
      "Running Average TRAIN F1-Score :  0.022714702211521366\n",
      "Running Average VAL Loss :  0.022900706606118808\n",
      "Running Average VAL F1-Score :  0.022015495219550627\n",
      "\n",
      "\n",
      "EPOCH  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:42<00:00, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.021942981867702094\n",
      "Running Average TRAIN F1-Score :  0.025105429978875608\n",
      "Running Average VAL Loss :  0.023011927692485706\n",
      "Running Average VAL F1-Score :  0.024634642243784453\n",
      "\n",
      "\n",
      "EPOCH  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:42<00:00, 23.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.021718724300483844\n",
      "Running Average TRAIN F1-Score :  0.027470543651239618\n",
      "Running Average VAL Loss :  0.02277990520399596\n",
      "Running Average VAL F1-Score :  0.024271585163660347\n",
      "\n",
      "\n",
      "EPOCH  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:44<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.021475006691315076\n",
      "Running Average TRAIN F1-Score :  0.029561574468491496\n",
      "Running Average VAL Loss :  0.022808618510940244\n",
      "Running Average VAL F1-Score :  0.028437050175853074\n",
      "\n",
      "\n",
      "EPOCH  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:43<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.02128240859115517\n",
      "Running Average TRAIN F1-Score :  0.03137576073169947\n",
      "Running Average VAL Loss :  0.022734716584506844\n",
      "Running Average VAL F1-Score :  0.025866824405966327\n",
      "\n",
      "\n",
      "EPOCH  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:45<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.021106562496243896\n",
      "Running Average TRAIN F1-Score :  0.03314184848187523\n",
      "Running Average VAL Loss :  0.022809570572072908\n",
      "Running Average VAL F1-Score :  0.027855467774705694\n",
      "\n",
      "\n",
      "EPOCH  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:46<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.0209420568325541\n",
      "Running Average TRAIN F1-Score :  0.035084994371187735\n",
      "Running Average VAL Loss :  0.022835598393742527\n",
      "Running Average VAL F1-Score :  0.02589314662951178\n",
      "\n",
      "\n",
      "EPOCH  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:46<00:00, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.019618871667376764\n",
      "Running Average TRAIN F1-Score :  0.04390733745637593\n",
      "Running Average VAL Loss :  0.02203022424198155\n",
      "Running Average VAL F1-Score :  0.036986984896689785\n",
      "\n",
      "\n",
      "EPOCH  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:45<00:00, 21.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.01922623904702904\n",
      "Running Average TRAIN F1-Score :  0.04723742898259606\n",
      "Running Average VAL Loss :  0.021984646446071565\n",
      "Running Average VAL F1-Score :  0.03876975501355316\n",
      "\n",
      "\n",
      "EPOCH  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:45<00:00, 22.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.019059916743292257\n",
      "Running Average TRAIN F1-Score :  0.04874469810998166\n",
      "Running Average VAL Loss :  0.022014415623354062\n",
      "Running Average VAL F1-Score :  0.039640678117783476\n",
      "\n",
      "\n",
      "EPOCH  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:45<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.01894065323141548\n",
      "Running Average TRAIN F1-Score :  0.049710659842406\n",
      "Running Average VAL Loss :  0.02199436811497435\n",
      "Running Average VAL F1-Score :  0.03870452526358089\n",
      "\n",
      "\n",
      "EPOCH  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 764/1001 [00:33<00:10, 23.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[81], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m train_dataloader, val_dataloader \u001B[38;5;241m=\u001B[39m get_train_val_dataloader(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mESM2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m t5_model, t5_losses, t5_scores \u001B[38;5;241m=\u001B[39m train_model(train_dataloader, val_dataloader, embeddings_source\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mESM2\u001B[39m\u001B[38;5;124m\"\u001B[39m, model_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[80], line 49\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(train_dataloader, val_dataloader, embeddings_source, model_type)\u001B[0m\n\u001B[0;32m     47\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     48\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 49\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     51\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(losses)\n\u001B[0;32m     52\u001B[0m avg_score \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(scores)\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    130\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    133\u001B[0m         group,\n\u001B[0;32m    134\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    138\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    139\u001B[0m         state_steps)\n\u001B[1;32m--> 141\u001B[0m     adam(\n\u001B[0;32m    142\u001B[0m         params_with_grad,\n\u001B[0;32m    143\u001B[0m         grads,\n\u001B[0;32m    144\u001B[0m         exp_avgs,\n\u001B[0;32m    145\u001B[0m         exp_avg_sqs,\n\u001B[0;32m    146\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    147\u001B[0m         state_steps,\n\u001B[0;32m    148\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    149\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[0;32m    150\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[0;32m    151\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    152\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    153\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    154\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    155\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    156\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    157\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    158\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    159\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    160\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    161\u001B[0m     )\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    279\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 281\u001B[0m func(params,\n\u001B[0;32m    282\u001B[0m      grads,\n\u001B[0;32m    283\u001B[0m      exp_avgs,\n\u001B[0;32m    284\u001B[0m      exp_avg_sqs,\n\u001B[0;32m    285\u001B[0m      max_exp_avg_sqs,\n\u001B[0;32m    286\u001B[0m      state_steps,\n\u001B[0;32m    287\u001B[0m      amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[0;32m    288\u001B[0m      beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[0;32m    289\u001B[0m      beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[0;32m    290\u001B[0m      lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[0;32m    291\u001B[0m      weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[0;32m    292\u001B[0m      eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[0;32m    293\u001B[0m      maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[0;32m    294\u001B[0m      capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[0;32m    295\u001B[0m      differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[0;32m    296\u001B[0m      grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[0;32m    297\u001B[0m      found_inf\u001B[38;5;241m=\u001B[39mfound_inf)\n",
      "File \u001B[1;32m~\\.conda\\envs\\light\\Lib\\site-packages\\torch\\optim\\adam.py:446\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m    445\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_mul_(device_exp_avgs, beta1)\n\u001B[1;32m--> 446\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_add_(device_exp_avgs, device_grads, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[0;32m    448\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001B[0;32m    449\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_train_val_dataloader(\"ESM2\")\n",
    "t5_model, t5_losses, t5_scores = train_model(train_dataloader, val_dataloader, embeddings_source=\"ESM2\", model_type=\"linear\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "EsmModel(\n  (embeddings): EsmEmbeddings(\n    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n    (dropout): Dropout(p=0.0, inplace=False)\n    (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n  )\n  (encoder): EsmEncoder(\n    (layer): ModuleList(\n      (0-32): 33 x EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=1280, out_features=1280, bias=True)\n            (key): Linear(in_features=1280, out_features=1280, bias=True)\n            (value): Linear(in_features=1280, out_features=1280, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (pooler): EsmPooler(\n    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n    (activation): Tanh()\n  )\n  (contact_head): EsmContactPredictionHead(\n    (regression): Linear(in_features=660, out_features=1, bias=True)\n    (activation): Sigmoid()\n  )\n)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t33_650M_UR50D')\n",
    "embedding_model = EsmModel.from_pretrained('facebook/esm2_t33_650M_UR50D', add_cross_attention=False, is_decoder=False)\n",
    "embedding_model.eval()\n",
    "embedding_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T04:08:32.213150600Z",
     "start_time": "2024-01-22T04:08:20.054141700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "ids = tokenizer(['MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN', 'QQQQQ'], add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])\n",
    "output = embedding_model(input_ids=input_ids, attention_mask=attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T04:08:35.758807900Z",
     "start_time": "2024-01-22T04:08:32.218148900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ True, False,  True, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False],\n       device='cuda:0')"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = t5_model(output.last_hidden_state[0].to(device))\n",
    "cls = t5_model(output.last_hidden_state[0][0].to(device))\n",
    "torch.nn.functional.sigmoid(cls) > 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T04:08:35.759808100Z",
     "start_time": "2024-01-22T04:08:35.462147100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t5_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(embeddings_source, data_source):\n",
    "    \"\"\"\n",
    "    Custom function to make inference using the model\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    \"\"\"\n",
    "\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "\n",
    "    if data_source == \"val\":\n",
    "        predict_dataloader = val_dataloader\n",
    "\n",
    "    if data_source == 'test':\n",
    "        predict_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"ESM2\":\n",
    "        model = esm2_model\n",
    "\n",
    "    # Set model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(predict_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_df = predict(\"T5\", \"val\")\n",
    "submission_df.to_tsv(\"/working/predictions_val.tsv\")\n",
    "submission_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### SCRIPT TO EVALUATE PREDICTIONS USING CAFA EVALUATOR ###\n",
    "\n",
    "import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval\n",
    "\n",
    "cafa_eval(f\"{DATA_DIR}/Train/go-basic.obo\", submission_df, f\"{DATA_DIR}/Train/train_terms.tsv\", ia=f\"{DATA_DIR}/IA.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### IN PROGRESS - SCRIPT TO TRAIN THE MODEL USING PyTorchLightning ###\n",
    "\n",
    "class Linear_Lightning(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    In progress, used to train the MLP model on multiple GPUs using PyTorchLightning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, train_size, **hparams):\n",
    "        super(Linear_Lightning, self).__init__()\n",
    "\n",
    "        self.model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "        train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "        self.train_set, self.val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n",
    "        self.accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        logs = {\"train_loss\" : loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss= self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        return {\"val_loss\": loss, \"f1_score\": f1_score, \"accuracy_score\": acc_score}\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in ouputs]).mean()\n",
    "        logs = {\"val_loss\" : avg_loss}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = torch.utils.data.DataLoader(self.val_set, batch_size=config.batch_size, shuffle=False,)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.n_epochs,\n",
    "    limit_train_batches=5000,\n",
    "    logger=logger)\n",
    "\n",
    "\n",
    "model = Linear_Lightning(\n",
    "    input_dim=embeds_dim[embeddings_source],\n",
    "    num_classes=config.num_labels,\n",
    "    train_size=0.8)\n",
    "\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
