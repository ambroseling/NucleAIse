{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.device)\n",
    "\n",
    "\"\"\"\n",
    "1. Test different loss functions (ambrose, better weights)\n",
    "2. Test different models (like Temporal CNN, bigger linear model (keeping track of hyperparameters)\n",
    "    https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "3. Implement CAFA-Evaluator for better metrics\n",
    "4. Use more GOs in predictions\n",
    "5. Read Kaggle notebooks online to gain intuition\n",
    "6. Use new data!\n",
    "7. Using description of each GO for making predictions rather than considering them as labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAIN_DIR = \"data/\"\n",
    "WORK_DIR = \"working/\"\n",
    "DATA_DIR = MAIN_DIR + \"cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"protbert-embeddings-for-cafa5\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load a sample submission for Kaggle competition\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define important configurations of the code\n",
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 500\n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    lr = 0.002\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "#\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "\n",
    "print(f\"GENERATE TARGETS FOR ENTRY IDS ({config.num_labels} MOST COMMON GO TERMS)\")\n",
    "ids = np.load(f\"{PROTBERT_DIR}/train_ids.npy\")\n",
    "labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "\n",
    "top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "labels_names = top_terms[:config.num_labels].index.values\n",
    "train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "for index, id in tqdm(enumerate(ids)):\n",
    "    id_gos_list = id_labels[id]\n",
    "    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "    labels_matrix[index, temp] = 1\n",
    "\n",
    "labels_list = []\n",
    "for l in range(labels_matrix.shape[0]):\n",
    "    labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "labels_df.to_pickle(f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "print(\"GENERATION FINISHED!\")\n",
    "labels_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GO_weight_dataset = pd.read_table(f'{DATA_DIR}/IA.txt', header=None, names=['GO', 'weight'])\n",
    "GO_weight_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load GO_weights (IA data) as a tensor to feed into the loss function\n",
    "\n",
    "GO_weights = []\n",
    "for each_label in labels_names:\n",
    "    GO_weights.append(GO_weight_dataset.loc[GO_weight_dataset['GO'] == each_label]['weight'].values[0])\n",
    "\n",
    "GO_weights = torch.tensor(GO_weights, dtype=torch.float32)\n",
    "GO_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"ESM2\" : \"cafa-5-esm-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"ESM2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to store embeddings of different sources\n",
    "    It could be used to get training or test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"ESM2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\\\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"T5\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline MLP model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, input_dim)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1000)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(1000, 800)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(800, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN-1D model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(embeddings_source, model_type, train_size=0.9):\n",
    "    \"\"\"\n",
    "    Custom function to train the baseline model on dataset\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    :param model_type: define the type of model\n",
    "    :param train_size: define the training portion ratio\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    model.to(config.device)\n",
    "\n",
    "    # define configurations of the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "\n",
    "    # multilabel prediction task, GO_weights could be used as weight in the loss function\n",
    "    # GO_weights.to(config.device)\n",
    "    MultiLabelLoss = torch.nn.BCEWithLogitsLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_f1score_history, val_f1score_history = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "\n",
    "        ## TRAIN PHASE :\n",
    "        model.train()\n",
    "        losses, scores = [], []\n",
    "\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            score = f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        model.eval()\n",
    "        losses, scores = [], []\n",
    "\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device),\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            score = f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "        avg_loss, avg_score = np.mean(losses), np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "\n",
    "        scheduler.step(avg_loss), print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"TRAINING FINISHED _____\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "\n",
    "    return model, losses_history, scores_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t5_model, t5_losses, t5_scores = train_model(embeddings_source=\"T5\", model_type=\"linear\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t5_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(embeddings_source):\n",
    "    \"\"\"\n",
    "    Custom function to make inference using the model\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    \"\"\"\n",
    "\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"ESM2\":\n",
    "        model = esm2_model\n",
    "\n",
    "    # Set model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(test_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = torch.nn.functional.sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_df = predict(\"T5\")\n",
    "submission_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Linear_Lightning(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    In progress, used to train the MLP model on multiple GPUs using PyTorchLightning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, train_size, **hparams):\n",
    "        super(Linear_Lightning, self).__init__()\n",
    "\n",
    "        self.model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "        train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "        self.train_set, self.val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n",
    "        self.accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        logs = {\"train_loss\" : loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss= self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        return {\"val_loss\": loss, \"f1_score\": f1_score, \"accuracy_score\": acc_score}\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in ouputs]).mean()\n",
    "        logs = {\"val_loss\" : avg_loss}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = torch.utils.data.DataLoader(self.val_set, batch_size=config.batch_size, shuffle=False,)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### IN PROGRESS - SCRIPT TO TRAIN THE MODEL USING PyTorchLightning ###\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.n_epochs,\n",
    "    limit_train_batches=5000,\n",
    "    logger=logger)\n",
    "\n",
    "model = Linear_Lightning(\n",
    "    input_dim=embeds_dim[embeddings_source],\n",
    "    num_classes=config.num_labels,\n",
    "    train_size=0.8\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
