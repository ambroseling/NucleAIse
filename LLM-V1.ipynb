{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAIN_DIR = \"data/\"\n",
    "WORK_DIR = \"working/\"\n",
    "DATA_DIR = MAIN_DIR + \"cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"protbert-embeddings-for-cafa5\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 500\n",
    "    n_epochs = 5\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "\n",
    "print(\"GENERATE TARGETS FOR ENTRY IDS (\"+str(config.num_labels)+\" MOST COMMON GO TERMS)\")\n",
    "ids = np.load(f\"{PROTBERT_DIR}/train_ids.npy\")\n",
    "labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "\n",
    "top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "labels_names = top_terms[:config.num_labels].index.values\n",
    "train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "for index, id in tqdm(enumerate(ids)):\n",
    "    id_gos_list = id_labels[id]\n",
    "    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "    labels_matrix[index, temp] = 1\n",
    "\n",
    "labels_list = []\n",
    "for l in range(labels_matrix.shape[0]):\n",
    "    labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "labels_df.to_pickle(f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "print(\"GENERATION FINISHED!\")\n",
    "labels_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"EMS2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T00:38:22.945893300Z",
     "start_time": "2023-11-05T00:38:22.937894500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "      EntryID                                              embed  \\\n0      Q9ZSA8  [-0.092291094, -0.066283956, -0.01226195, 0.04...   \n1      P25353  [0.011624349, -0.030317612, -0.0058019715, 0.0...   \n2  A0A2R8YCW8  [0.02737274, -0.041047025, -0.029205365, 0.028...   \n3      G3V5N8  [0.033766113, -0.07888931, -0.05974137, 0.0456...   \n4  A0A140LFN4  [0.0119482, -0.002107593, -0.084922194, 0.0687...   \n5      B8ZZU6  [0.020136692, -0.13927373, -0.04045331, 0.0626...   \n6      Q01850  [0.026013047, -0.08974387, -0.020240448, 0.132...   \n7      P11076  [0.005006821, -0.03306428, -0.037696116, 0.059...   \n8      Q9VJ64  [0.029283574, -0.010495956, -0.013060905, 0.02...   \n9      Q7YSJ4  [0.02852764, -0.049777817, -0.056415968, 0.141...   \n\n                                         labels_vect  \n0  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n1  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n2  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n3  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n4  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n5  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n8  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n9  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>embed</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q9ZSA8</td>\n      <td>[-0.092291094, -0.066283956, -0.01226195, 0.04...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P25353</td>\n      <td>[0.011624349, -0.030317612, -0.0058019715, 0.0...</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A2R8YCW8</td>\n      <td>[0.02737274, -0.041047025, -0.029205365, 0.028...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>G3V5N8</td>\n      <td>[0.033766113, -0.07888931, -0.05974137, 0.0456...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A140LFN4</td>\n      <td>[0.0119482, -0.002107593, -0.084922194, 0.0687...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>B8ZZU6</td>\n      <td>[0.020136692, -0.13927373, -0.04045331, 0.0626...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Q01850</td>\n      <td>[0.026013047, -0.08974387, -0.020240448, 0.132...</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>P11076</td>\n      <td>[0.005006821, -0.03306428, -0.037696116, 0.059...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Q9VJ64</td>\n      <td>[0.029283574, -0.010495956, -0.013060905, 0.02...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Q7YSJ4</td>\n      <td>[0.02852764, -0.049777817, -0.056415968, 0.141...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\\\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"EMS2\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T02:08:26.460742700Z",
     "start_time": "2023-11-05T02:08:23.791986400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPONENTS FOR FIRST PROTEIN:  \n",
      "EMBEDDINGS VECTOR: \n",
      "  tensor([-0.0923, -0.0663, -0.0123,  ..., -0.1607,  0.0159,  0.0017]) \n",
      "\n",
      "TARGETS LABELS VECTOR: \n",
      "  tensor([0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T02:08:27.485047400Z",
     "start_time": "2023-11-05T02:08:27.479131800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1012)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(1012, 712)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(712, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T00:47:17.718261100Z",
     "start_time": "2023-11-05T00:47:17.701260500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T00:51:26.467833900Z",
     "start_time": "2023-11-05T00:51:26.449834600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:25<00:00, 39.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  137.9258147400695\n",
      "Running Average TRAIN F1-Score :  0.10282804407960885\n",
      "Running Average VAL Loss :  132.60320098059518\n",
      "Running Average VAL F1-Score :  0.15096353127488069\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:29<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  132.27895705826157\n",
      "Running Average TRAIN F1-Score :  0.16168885272401912\n",
      "Running Average VAL Loss :  129.6351577895028\n",
      "Running Average VAL F1-Score :  0.16599159840760486\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:24<00:00, 40.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  130.36139335975304\n",
      "Running Average TRAIN F1-Score :  0.1730289383382945\n",
      "Running Average VAL Loss :  128.99249730791365\n",
      "Running Average VAL F1-Score :  0.17349233997187444\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:28<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  129.27799826211387\n",
      "Running Average TRAIN F1-Score :  0.18054748853514124\n",
      "Running Average VAL Loss :  127.80783530644008\n",
      "Running Average VAL F1-Score :  0.1819651039051158\n",
      "\n",
      "\n",
      "EPOCH  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:24<00:00, 40.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  128.49565284568948\n",
      "Running Average TRAIN F1-Score :  0.1859274588145695\n",
      "Running Average VAL Loss :  127.0153968674796\n",
      "Running Average VAL F1-Score :  0.185508558260543\n",
      "\n",
      "\n",
      "TRAINING FINISHED\n",
      "FINAL TRAINING SCORE :  0.1859274588145695\n",
      "FINAL VALIDATION SCORE :  0.185508558260543\n"
     ]
    }
   ],
   "source": [
    "def train_model(embeddings_source, model_type=\"linear\", train_size=0.9):\n",
    "\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "\n",
    "    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "    CrossEntropy = torch.nn.CrossEntropyLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "\n",
    "    train_f1score_history=[]\n",
    "    val_f1score_history=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "        ## TRAIN PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"TRAINING FINISHED\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "\n",
    "    return model, losses_history, scores_history\n",
    "\n",
    "\n",
    "ems2_model, ems2_losses, ems2_scores = train_model(embeddings_source=\"EMS2\", model_type=\"conv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T01:05:11.986698600Z",
     "start_time": "2023-11-05T01:02:42.564527300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.3614,  4.4422,  3.3582,  4.4130,  3.5421,  4.0963,  3.2633,  3.2357,\n          2.6108,  3.1686,  3.3675,  3.1777,  2.2656,  2.2527,  1.6129,  1.1627,\n          3.4669,  4.2954,  2.3734,  2.2344,  2.4893,  4.0412,  2.5327,  1.0220,\n          4.4346,  4.0681,  3.2098,  2.4389,  2.6691,  1.3291,  0.3863,  0.3933,\n          0.0571,  0.1333,  2.7415,  0.2767,  2.2885, -0.7042,  2.2657,  1.0918,\n         -2.0445,  0.1190,  0.1533, -0.7778, -0.7773, -0.7578,  0.2327, -0.5024,\n          0.0200, -0.0803,  3.5225,  2.5142, -0.5563,  2.6985, -0.9831, -0.8499,\n          1.1326, -0.9097,  0.9827,  1.3431, -2.7158,  0.8884,  1.2357,  0.9186,\n         -0.6056,  3.4156, -1.1062,  1.7882,  0.7484,  2.0584,  0.0669,  2.9543,\n         -1.1269,  3.6058, -0.3335,  2.8464, -1.8762, -1.0112,  3.5109,  0.6140,\n         -0.8772, -1.1279,  0.4499, -1.8721,  3.3256,  2.7646, -0.1361, -1.1882,\n         -1.2810, -1.3150,  3.4049,  0.1128,  0.5204, -1.0620, -1.0783, -1.1083,\n         -2.1880, -0.0204,  0.0727,  1.3154, -3.3693,  1.2135, -0.6199, -0.6266,\n         -0.6114,  0.2199, -0.4634,  0.7535,  1.5243,  0.2507,  1.0657, -0.4126,\n         -1.5606, -1.0012, -2.2080,  3.1375, -2.2946, -0.4942, -2.8203,  2.2402,\n         -1.2231, -1.3761,  2.5898, -1.1773, -1.7880, -0.7688,  1.4763, -0.3770,\n          3.1736, -2.3007, -0.4821,  1.2823,  0.4703, -1.9896,  1.5711, -0.9731,\n          1.7534, -3.7008, -1.9032,  1.5539,  1.5106,  1.5928, -0.7970, -2.4123,\n          0.5956,  0.8738, -2.4052,  0.4118, -4.6549, -2.2469, -1.9769, -1.6312,\n         -1.4524, -1.5318,  1.5672, -1.5209,  0.3737, -2.7885,  1.9569, -2.6368,\n         -1.8863, -2.5463,  0.3911,  2.9209, -4.8795, -2.7267, -0.9054, -4.8988,\n          1.6021,  0.9359, -3.8707, -0.1520, -2.9978, -4.5229, -0.9964, -2.9232,\n         -3.3154, -5.0793, -1.7217, -3.2850, -2.8287,  2.2193,  4.3075, -5.3676,\n          1.1891, -2.7949, -2.1347, -3.9364, -1.4998, -5.8480, -0.5574, -0.2832,\n          2.8811, -1.3617,  2.8913, -4.4582,  0.8562,  2.8533, -5.4878, -2.8558,\n         -0.6280, -2.8220, -2.8298, -2.5988, -3.3123, -3.1958, -0.6124, -1.3493,\n         -2.4028, -0.9479, -1.5159,  2.6581, -1.5960, -5.8506, -1.2835, -3.2496,\n         -0.2886, -1.8866, -2.3510, -1.4177, -3.4078,  1.9996, -0.9383, -3.4451,\n         -2.2836, -5.0627, -0.1210, -1.7420, -1.8072, -2.6374, -4.3405, -1.9575,\n         -2.1378, -2.0669, -2.4057,  1.5221, -2.7166, -4.7648, -1.8384,  2.2845,\n         -1.2544,  0.5856,  2.5928,  0.6939, -2.3847,  2.9522,  0.6635, -0.5685,\n         -6.1630, -2.4510, -4.0490, -5.4752, -4.2946, -3.0503, -3.1475, -4.8960,\n         -1.4130, -1.5506,  1.0679, -1.3556, -5.1186, -3.8112,  2.9126,  1.5242,\n         -1.5829, -1.6095,  0.7874, -0.3047, -1.6675,  1.1302, -1.8544, -3.5824,\n         -2.0704, -1.8664, -0.3559, -2.1089, -5.4196, -1.8262, -1.8238, -5.1078,\n         -2.8696, -5.6924, -2.7696, -0.1352, -2.8590,  0.5974, -1.9436, -1.7080,\n         -4.7177, -4.2106, -3.5813, -3.3773, -0.9142, -0.3345, -4.6169, -4.2333,\n         -2.5533,  0.3666, -4.6865, -2.5873, -5.2607,  1.1093, -3.5718, -0.1167,\n         -2.6581, -3.6987, -1.1966,  0.3110, -3.1543, -5.1852,  0.0364, -3.9155,\n         -0.1861, -4.0443, -1.8567, -3.5371, -3.0993, -1.4704, -2.4296,  0.6148,\n          2.0788, -5.7211, -3.0698,  0.5635, -1.6106, -0.9796, -5.5356,  1.5633,\n         -9.5292, -3.1587, -3.5716,  0.6679, -2.5306, -2.2904, -3.7349, -2.9746,\n         -1.5288, -7.0687, -4.3859, -6.0268, -4.5950, -4.2122,  2.3278, -0.7000,\n         -5.2066, -2.7878, -5.6939, -1.9921, -2.9900, -5.8912, -4.1512, -1.3739,\n         -6.8405, -3.3740, -3.3055,  0.5095, -2.3959,  0.0187, -1.2904, -3.6194,\n          2.0024, -0.8246,  2.0783,  0.3346, -5.1948, -3.9677, -2.3629, -3.0351,\n         -2.4832, -2.6780, -3.0843, -5.4586, -4.0046, -3.0992, -5.1688, -5.1673,\n         -4.7132, -5.1117, -5.6235, -2.7082, -5.3253, -1.7411, -4.5278, -5.0709,\n         -5.0133,  1.1155, -5.1765, -5.3942, -2.9930, -2.9521, -6.0871, -1.3610,\n         -5.2883,  0.1896, -2.9722,  1.5767, -7.8758, -5.0807, -5.5448, -2.2927,\n         -3.2206, -3.5142, -2.3662, -5.1349,  0.2970, -6.0199, -2.5777,  2.3000,\n         -4.5690, -4.3557, -2.3988, -4.4429, -0.7440, -0.6416, -3.5522, -2.6332,\n         -2.8310, -4.0036,  0.4438, -0.3451, -5.1026, -4.8830, -7.3394, -0.1352,\n         -2.3693, -4.4340, -3.0366, -7.8861, -4.5053, -2.8738, -5.5212, -3.6219,\n         -2.0620, -4.3489, -4.7942, -4.3504, -3.2606, -5.8185, -5.2217, -3.2327,\n         -3.7225, -5.8436,  2.7579, -0.8171, -3.1883, -4.6451, -2.6623, -5.5194,\n         -2.3991, -3.2958, -4.7020,  1.2473, -3.1722, -2.3172, -7.0901, -2.7520,\n         -5.4157, -3.6407, -0.2680, -1.9016, -4.9663, -3.9728, -3.9756, -4.5542,\n         -4.4700, -6.7194, -2.7746, -1.4931,  0.5183, -4.3015, -4.6278, -4.2386,\n         -5.0798, -4.5299, -4.5698, -1.5686,  1.0253, -4.3284, -5.3775, -7.9645,\n         -4.6421, -7.9538, -3.4660, -5.2300, -0.0207, -1.5535, -3.0624, -2.0647,\n         -2.8684, -4.2569, -3.9424, -4.6187, -5.2513, -3.1493, -4.8430, -1.5734,\n          0.3037,  1.9390, -5.8130, -5.9212]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ems2_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T02:09:10.973831600Z",
     "start_time": "2023-11-05T02:09:10.862067200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
