{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:03:16.348596300Z",
     "start_time": "2023-11-13T00:03:04.236815600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adibv\\AppData\\Local\\Temp\\ipykernel_39696\\3520400249.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n",
      "C:\\Users\\adibv\\.conda\\envs\\light\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "'NVIDIA GeForce GTX 1650 Ti'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.cuda.get_device_name(torch.cuda.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cafa-5-ems-2-embeddings-numpy\\test_embeddings.npy\n",
      "data/cafa-5-ems-2-embeddings-numpy\\test_ids.npy\n",
      "data/cafa-5-ems-2-embeddings-numpy\\train_embeddings.npy\n",
      "data/cafa-5-ems-2-embeddings-numpy\\train_ids.npy\n",
      "data/cafa-5-protein-function-prediction\\IA.txt\n",
      "data/cafa-5-protein-function-prediction\\sample_submission.tsv\n",
      "data/cafa-5-protein-function-prediction\\Test (Targets)\\testsuperset-taxon-list.tsv\n",
      "data/cafa-5-protein-function-prediction\\Test (Targets)\\testsuperset.fasta\n",
      "data/cafa-5-protein-function-prediction\\Train\\go-basic.obo\n",
      "data/cafa-5-protein-function-prediction\\Train\\train_sequences.fasta\n",
      "data/cafa-5-protein-function-prediction\\Train\\train_taxonomy.tsv\n",
      "data/cafa-5-protein-function-prediction\\Train\\train_terms.tsv\n",
      "data/protbert-embeddings-for-cafa5\\test_embeddings.npy\n",
      "data/protbert-embeddings-for-cafa5\\test_ids.npy\n",
      "data/protbert-embeddings-for-cafa5\\train_embeddings.npy\n",
      "data/protbert-embeddings-for-cafa5\\train_ids.npy\n",
      "data/t5embeds\\test_embeds.npy\n",
      "data/t5embeds\\test_ids.npy\n",
      "data/t5embeds\\train_embeds.npy\n",
      "data/t5embeds\\train_ids.npy\n"
     ]
    }
   ],
   "source": [
    "MAIN_DIR = \"data/\"\n",
    "WORK_DIR = \"working/\"\n",
    "DATA_DIR = MAIN_DIR + \"cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"protbert-embeddings-for-cafa5\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:03:16.354992Z",
     "start_time": "2023-11-13T00:03:16.349599200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "    ProteinID       GO_ID  Probability\n0  A0A0A0MRZ7  GO:0000001        0.123\n1  A0A0A0MRZ7  GO:0000002        0.123\n2  A0A0A0MRZ8  GO:0000001        0.123\n3  A0A0A0MRZ8  GO:0000002        0.123\n4  A0A0A0MRZ9  GO:0000001        0.123\n5  A0A0A0MRZ9  GO:0000002        0.123\n6  A0A0A0MS00  GO:0000001        0.123\n7  A0A0A0MS00  GO:0000002        0.123\n8  A0A0A0MS01  GO:0000001        0.123\n9  A0A0A0MS01  GO:0000002        0.123",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ProteinID</th>\n      <th>GO_ID</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A0A0MRZ7</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A0A0MRZ7</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A0A0MRZ8</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0A0MRZ8</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A0A0MRZ9</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A0A0A0MRZ9</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>A0A0A0MS00</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>A0A0A0MS00</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>A0A0A0MS01</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A0A0A0MS01</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:03:16.532074300Z",
     "start_time": "2023-11-13T00:03:16.356992500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda - NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 500\n",
    "    n_epochs = 25\n",
    "    batch_size = 128\n",
    "    lr = 0.002\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:11:24.648628300Z",
     "start_time": "2023-11-13T00:11:24.607631300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE TARGETS FOR ENTRY IDS (500 MOST COMMON GO TERMS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142246it [00:37, 3829.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION FINISHED!\n"
     ]
    },
    {
     "data": {
      "text/plain": "      EntryID                                        labels_vect\n0      P20536  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...\n1      O73864  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...\n2      O95231  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...\n3  A0A0B4J1F4  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...\n4      P54366  [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P20536</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O73864</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O95231</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0B4J1F4</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P54366</td>\n      <td>[1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "\n",
    "print(\"GENERATE TARGETS FOR ENTRY IDS (\"+str(config.num_labels)+\" MOST COMMON GO TERMS)\")\n",
    "ids = np.load(f\"{PROTBERT_DIR}/train_ids.npy\")\n",
    "labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "\n",
    "top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "labels_names = top_terms[:config.num_labels].index.values\n",
    "train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "for index, id in tqdm(enumerate(ids)):\n",
    "    id_gos_list = id_labels[id]\n",
    "    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "    labels_matrix[index, temp] = 1\n",
    "\n",
    "labels_list = []\n",
    "for l in range(labels_matrix.shape[0]):\n",
    "    labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "labels_df.to_pickle(f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "print(\"GENERATION FINISHED!\")\n",
    "labels_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:04:13.425971600Z",
     "start_time": "2023-11-13T00:03:26.674597800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"EMS2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:04:13.433113200Z",
     "start_time": "2023-11-13T00:04:13.428970900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "      EntryID                                              embed  \\\n0      P20536  [0.04948842525482178, -0.03293515741825104, 0....   \n1      O73864  [-0.04461636394262314, 0.06492499262094498, -0...   \n2      O95231  [-0.02012803591787815, -0.04977943375706673, 0...   \n3  A0A0B4J1F4  [-0.00751461973413825, 0.06062775477766991, 0....   \n4      P54366  [0.013468174263834953, 0.04151567816734314, 0....   \n5      P33681  [0.001116646104492247, -0.01536268275231123, 0...   \n6      P77596  [0.03678780049085617, 0.052980050444602966, 0....   \n7      Q16787  [0.007108339574187994, 0.01562744379043579, 0....   \n8      Q59VP0  [-0.006104866974055767, -0.026720179244875908,...   \n9      P13508  [-0.0071898759342730045, -0.02323203906416893,...   \n\n                                         labels_vect  \n0  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n1  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n2  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n3  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...  \n4  [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n5  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n6  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n7  [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...  \n8  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n9  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>embed</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P20536</td>\n      <td>[0.04948842525482178, -0.03293515741825104, 0....</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O73864</td>\n      <td>[-0.04461636394262314, 0.06492499262094498, -0...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O95231</td>\n      <td>[-0.02012803591787815, -0.04977943375706673, 0...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0B4J1F4</td>\n      <td>[-0.00751461973413825, 0.06062775477766991, 0....</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P54366</td>\n      <td>[0.013468174263834953, 0.04151567816734314, 0....</td>\n      <td>[1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>P33681</td>\n      <td>[0.001116646104492247, -0.01536268275231123, 0...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>P77596</td>\n      <td>[0.03678780049085617, 0.052980050444602966, 0....</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Q16787</td>\n      <td>[0.007108339574187994, 0.01562744379043579, 0....</td>\n      <td>[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Q59VP0</td>\n      <td>[-0.006104866974055767, -0.026720179244875908,...</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>P13508</td>\n      <td>[-0.0071898759342730045, -0.02323203906416893,...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{WORK_DIR}/train_targets_top\"+str(config.num_labels)+\".pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\\\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"T5\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:04:42.075324400Z",
     "start_time": "2023-11-13T00:04:38.745475100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPONENTS FOR FIRST PROTEIN:  \n",
      "EMBEDDINGS VECTOR: \n",
      "  tensor([ 0.0495, -0.0329,  0.0325,  ..., -0.0435,  0.0965,  0.0731]) \n",
      "\n",
      "TARGETS LABELS VECTOR: \n",
      "  tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:04:42.247602700Z",
     "start_time": "2023-11-13T00:04:42.081847600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, input_dim)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1000)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(1000, 800)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(800, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:16:03.076042700Z",
     "start_time": "2023-11-13T00:16:03.059043900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:04:52.537968100Z",
     "start_time": "2023-11-13T00:04:52.518372Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(embeddings_source, model_type=\"linear\", train_size=0.9):\n",
    "\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "\n",
    "    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "    MultiLabelLoss = torch.nn.BCEWithLogitsLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "\n",
    "    train_f1score_history=[]\n",
    "    val_f1score_history=[]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "\n",
    "        ## TRAIN PHASE :\n",
    "        losses, scores = [], []\n",
    "\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "            loss= MultiLabelLoss(preds, targets)\n",
    "\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        losses, scores = [], []\n",
    "\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss= MultiLabelLoss(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"TRAINING FINISHED\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "\n",
    "    return model, losses_history, scores_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:38<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.14723475045942283\n",
      "Running Average TRAIN F1-Score :  0.0372122754938305\n",
      "Running Average VAL Loss :  0.13460040618000285\n",
      "Running Average VAL F1-Score :  0.0732388557433816\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:38<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.13037609190612168\n",
      "Running Average TRAIN F1-Score :  0.0806840056596281\n",
      "Running Average VAL Loss :  0.12976928959999764\n",
      "Running Average VAL F1-Score :  0.09229968850766974\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:37<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.12601814652001345\n",
      "Running Average TRAIN F1-Score :  0.10553580846685867\n",
      "Running Average VAL Loss :  0.1273318100720644\n",
      "Running Average VAL F1-Score :  0.10245534395133811\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:37<00:00, 26.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.12247991531372784\n",
      "Running Average TRAIN F1-Score :  0.12793126974727487\n",
      "Running Average VAL Loss :  0.12556536103199636\n",
      "Running Average VAL F1-Score :  0.1269398009005402\n",
      "\n",
      "\n",
      "EPOCH  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:38<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.11940935002518939\n",
      "Running Average TRAIN F1-Score :  0.1490349325460273\n",
      "Running Average VAL Loss :  0.12478991903896842\n",
      "Running Average VAL F1-Score :  0.1656294320044773\n",
      "\n",
      "\n",
      "EPOCH  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:38<00:00, 26.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.11648161369157242\n",
      "Running Average TRAIN F1-Score :  0.16883610393647308\n",
      "Running Average VAL Loss :  0.12283315349902425\n",
      "Running Average VAL F1-Score :  0.15888071632278816\n",
      "\n",
      "\n",
      "EPOCH  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:37<00:00, 26.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.11356009525078517\n",
      "Running Average TRAIN F1-Score :  0.18733627639420622\n",
      "Running Average VAL Loss :  0.12277722804407988\n",
      "Running Average VAL F1-Score :  0.17766014406723635\n",
      "\n",
      "\n",
      "EPOCH  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:39<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.11078822561404802\n",
      "Running Average TRAIN F1-Score :  0.20694158645300242\n",
      "Running Average VAL Loss :  0.12223972473293543\n",
      "Running Average VAL F1-Score :  0.19981384024556195\n",
      "\n",
      "\n",
      "EPOCH  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:37<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.10804230017172588\n",
      "Running Average TRAIN F1-Score :  0.225697842198652\n",
      "Running Average VAL Loss :  0.12107926447476659\n",
      "Running Average VAL F1-Score :  0.19289390676255738\n",
      "\n",
      "\n",
      "EPOCH  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:39<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.10532545226883817\n",
      "Running Average TRAIN F1-Score :  0.2443946681983702\n",
      "Running Average VAL Loss :  0.12118569076327342\n",
      "Running Average VAL F1-Score :  0.19555765683097498\n",
      "\n",
      "\n",
      "EPOCH  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:38<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  0.10278876154840767\n",
      "Running Average TRAIN F1-Score :  0.26213499251660055\n"
     ]
    }
   ],
   "source": [
    "t5_model, t5_losses, t5_scores = train_model(embeddings_source=\"T5\", model_type=\"linear\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-13T00:16:08.281846800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.3614,  4.4422,  3.3582,  4.4130,  3.5421,  4.0963,  3.2633,  3.2357,\n          2.6108,  3.1686,  3.3675,  3.1777,  2.2656,  2.2527,  1.6129,  1.1627,\n          3.4669,  4.2954,  2.3734,  2.2344,  2.4893,  4.0412,  2.5327,  1.0220,\n          4.4346,  4.0681,  3.2098,  2.4389,  2.6691,  1.3291,  0.3863,  0.3933,\n          0.0571,  0.1333,  2.7415,  0.2767,  2.2885, -0.7042,  2.2657,  1.0918,\n         -2.0445,  0.1190,  0.1533, -0.7778, -0.7773, -0.7578,  0.2327, -0.5024,\n          0.0200, -0.0803,  3.5225,  2.5142, -0.5563,  2.6985, -0.9831, -0.8499,\n          1.1326, -0.9097,  0.9827,  1.3431, -2.7158,  0.8884,  1.2357,  0.9186,\n         -0.6056,  3.4156, -1.1062,  1.7882,  0.7484,  2.0584,  0.0669,  2.9543,\n         -1.1269,  3.6058, -0.3335,  2.8464, -1.8762, -1.0112,  3.5109,  0.6140,\n         -0.8772, -1.1279,  0.4499, -1.8721,  3.3256,  2.7646, -0.1361, -1.1882,\n         -1.2810, -1.3150,  3.4049,  0.1128,  0.5204, -1.0620, -1.0783, -1.1083,\n         -2.1880, -0.0204,  0.0727,  1.3154, -3.3693,  1.2135, -0.6199, -0.6266,\n         -0.6114,  0.2199, -0.4634,  0.7535,  1.5243,  0.2507,  1.0657, -0.4126,\n         -1.5606, -1.0012, -2.2080,  3.1375, -2.2946, -0.4942, -2.8203,  2.2402,\n         -1.2231, -1.3761,  2.5898, -1.1773, -1.7880, -0.7688,  1.4763, -0.3770,\n          3.1736, -2.3007, -0.4821,  1.2823,  0.4703, -1.9896,  1.5711, -0.9731,\n          1.7534, -3.7008, -1.9032,  1.5539,  1.5106,  1.5928, -0.7970, -2.4123,\n          0.5956,  0.8738, -2.4052,  0.4118, -4.6549, -2.2469, -1.9769, -1.6312,\n         -1.4524, -1.5318,  1.5672, -1.5209,  0.3737, -2.7885,  1.9569, -2.6368,\n         -1.8863, -2.5463,  0.3911,  2.9209, -4.8795, -2.7267, -0.9054, -4.8988,\n          1.6021,  0.9359, -3.8707, -0.1520, -2.9978, -4.5229, -0.9964, -2.9232,\n         -3.3154, -5.0793, -1.7217, -3.2850, -2.8287,  2.2193,  4.3075, -5.3676,\n          1.1891, -2.7949, -2.1347, -3.9364, -1.4998, -5.8480, -0.5574, -0.2832,\n          2.8811, -1.3617,  2.8913, -4.4582,  0.8562,  2.8533, -5.4878, -2.8558,\n         -0.6280, -2.8220, -2.8298, -2.5988, -3.3123, -3.1958, -0.6124, -1.3493,\n         -2.4028, -0.9479, -1.5159,  2.6581, -1.5960, -5.8506, -1.2835, -3.2496,\n         -0.2886, -1.8866, -2.3510, -1.4177, -3.4078,  1.9996, -0.9383, -3.4451,\n         -2.2836, -5.0627, -0.1210, -1.7420, -1.8072, -2.6374, -4.3405, -1.9575,\n         -2.1378, -2.0669, -2.4057,  1.5221, -2.7166, -4.7648, -1.8384,  2.2845,\n         -1.2544,  0.5856,  2.5928,  0.6939, -2.3847,  2.9522,  0.6635, -0.5685,\n         -6.1630, -2.4510, -4.0490, -5.4752, -4.2946, -3.0503, -3.1475, -4.8960,\n         -1.4130, -1.5506,  1.0679, -1.3556, -5.1186, -3.8112,  2.9126,  1.5242,\n         -1.5829, -1.6095,  0.7874, -0.3047, -1.6675,  1.1302, -1.8544, -3.5824,\n         -2.0704, -1.8664, -0.3559, -2.1089, -5.4196, -1.8262, -1.8238, -5.1078,\n         -2.8696, -5.6924, -2.7696, -0.1352, -2.8590,  0.5974, -1.9436, -1.7080,\n         -4.7177, -4.2106, -3.5813, -3.3773, -0.9142, -0.3345, -4.6169, -4.2333,\n         -2.5533,  0.3666, -4.6865, -2.5873, -5.2607,  1.1093, -3.5718, -0.1167,\n         -2.6581, -3.6987, -1.1966,  0.3110, -3.1543, -5.1852,  0.0364, -3.9155,\n         -0.1861, -4.0443, -1.8567, -3.5371, -3.0993, -1.4704, -2.4296,  0.6148,\n          2.0788, -5.7211, -3.0698,  0.5635, -1.6106, -0.9796, -5.5356,  1.5633,\n         -9.5292, -3.1587, -3.5716,  0.6679, -2.5306, -2.2904, -3.7349, -2.9746,\n         -1.5288, -7.0687, -4.3859, -6.0268, -4.5950, -4.2122,  2.3278, -0.7000,\n         -5.2066, -2.7878, -5.6939, -1.9921, -2.9900, -5.8912, -4.1512, -1.3739,\n         -6.8405, -3.3740, -3.3055,  0.5095, -2.3959,  0.0187, -1.2904, -3.6194,\n          2.0024, -0.8246,  2.0783,  0.3346, -5.1948, -3.9677, -2.3629, -3.0351,\n         -2.4832, -2.6780, -3.0843, -5.4586, -4.0046, -3.0992, -5.1688, -5.1673,\n         -4.7132, -5.1117, -5.6235, -2.7082, -5.3253, -1.7411, -4.5278, -5.0709,\n         -5.0133,  1.1155, -5.1765, -5.3942, -2.9930, -2.9521, -6.0871, -1.3610,\n         -5.2883,  0.1896, -2.9722,  1.5767, -7.8758, -5.0807, -5.5448, -2.2927,\n         -3.2206, -3.5142, -2.3662, -5.1349,  0.2970, -6.0199, -2.5777,  2.3000,\n         -4.5690, -4.3557, -2.3988, -4.4429, -0.7440, -0.6416, -3.5522, -2.6332,\n         -2.8310, -4.0036,  0.4438, -0.3451, -5.1026, -4.8830, -7.3394, -0.1352,\n         -2.3693, -4.4340, -3.0366, -7.8861, -4.5053, -2.8738, -5.5212, -3.6219,\n         -2.0620, -4.3489, -4.7942, -4.3504, -3.2606, -5.8185, -5.2217, -3.2327,\n         -3.7225, -5.8436,  2.7579, -0.8171, -3.1883, -4.6451, -2.6623, -5.5194,\n         -2.3991, -3.2958, -4.7020,  1.2473, -3.1722, -2.3172, -7.0901, -2.7520,\n         -5.4157, -3.6407, -0.2680, -1.9016, -4.9663, -3.9728, -3.9756, -4.5542,\n         -4.4700, -6.7194, -2.7746, -1.4931,  0.5183, -4.3015, -4.6278, -4.2386,\n         -5.0798, -4.5299, -4.5698, -1.5686,  1.0253, -4.3284, -5.3775, -7.9645,\n         -4.6421, -7.9538, -3.4660, -5.2300, -0.0207, -1.5535, -3.0624, -2.0647,\n         -2.8684, -4.2569, -3.9424, -4.6187, -5.2513, -3.1493, -4.8430, -1.5734,\n          0.3037,  1.9390, -5.8130, -5.9212]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T02:09:10.973831600Z",
     "start_time": "2023-11-05T02:09:10.862067200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(embeddings_source):\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"EMS2\":\n",
    "        model = ems2_model\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(test_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = torch.nn.functional.sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_df = predict(\"T5\")\n",
    "submission_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
