{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:08:28.982667500Z",
     "start_time": "2024-02-24T19:08:06.896636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adibv\\.conda\\envs\\light\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "C:\\Users\\adibv\\AppData\\Local\\Temp\\ipykernel_23188\\2762250000.py:23: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n1. Test different loss functions (ambrose, better weights)\\n2. Test different models (like Temporal CNN, bigger linear model (keeping track of hyperparameters)\\n    https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\\n3. Implement CAFA-Evaluator for better metrics\\n4. Use more GOs in predictions\\n5. Read Kaggle notebooks online to gain intuition\\n6. Use new data!\\n7. Using description of each GO for making predictions rather than considering them as labels\\n8. Implement winning Kaggle models\\n9. Debug current code\\n10. Use taxonomy (one-hot encoded, embedded)\\n\\n*** Add more information from the Kaggle + Article stuff to the powerpoint\\n*** Create a schema of what model we want to create\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "torch.cuda.get_device_name(torch.cuda.device)\n",
    "\n",
    "\"\"\"\n",
    "1. Test different loss functions (ambrose, better weights)\n",
    "2. Test different models (like Temporal CNN, bigger linear model (keeping track of hyperparameters)\n",
    "    https://unit8.com/resources/temporal-convolutional-networks-and-forecasting/\n",
    "3. Implement CAFA-Evaluator for better metrics\n",
    "4. Use more GOs in predictions\n",
    "5. Read Kaggle notebooks online to gain intuition\n",
    "6. Use new data!\n",
    "7. Using description of each GO for making predictions rather than considering them as labels\n",
    "8. Implement winning Kaggle models\n",
    "9. Debug current code\n",
    "10. Use taxonomy (one-hot encoded, embedded)\n",
    "\n",
    "*** Add more information from the Kaggle + Article stuff to the powerpoint\n",
    "*** Create a schema of what model we want to create\n",
    "\"\"\"\n",
    "\n",
    "# esm2_t33_650M_UR50D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\cafa-5-esm-2-embeddings-numpy\\go_weights_10000.pt\n",
      "data\\cafa-5-esm-2-embeddings-numpy\\test_embeddings.npy\n",
      "data\\cafa-5-esm-2-embeddings-numpy\\test_ids.npy\n",
      "data\\cafa-5-esm-2-embeddings-numpy\\train_embeddings.npy\n",
      "data\\cafa-5-esm-2-embeddings-numpy\\train_ids.npy\n",
      "data\\cafa-5-esm-2-embeddings-numpy\\train_targets_top10000.pkl\n",
      "data\\cafa-5-esm-2-embeddings-numpy\\train_targets_top5000.pkl\n",
      "data\\cafa-5-protein-function-prediction\\IA.txt\n",
      "data\\cafa-5-protein-function-prediction\\sample_submission.tsv\n",
      "data\\cafa-5-protein-function-prediction\\Test (Targets)\\testsuperset-taxon-list.tsv\n",
      "data\\cafa-5-protein-function-prediction\\Test (Targets)\\testsuperset.fasta\n",
      "data\\cafa-5-protein-function-prediction\\Train\\go-basic.obo\n",
      "data\\cafa-5-protein-function-prediction\\Train\\train_sequences.fasta\n",
      "data\\cafa-5-protein-function-prediction\\Train\\train_taxonomy.tsv\n",
      "data\\cafa-5-protein-function-prediction\\Train\\train_terms.tsv\n",
      "data\\protbert-embeddings-for-cafa5\\test_embeddings.npy\n",
      "data\\protbert-embeddings-for-cafa5\\test_ids.npy\n",
      "data\\protbert-embeddings-for-cafa5\\train_embeddings.npy\n",
      "data\\protbert-embeddings-for-cafa5\\train_ids.npy\n",
      "data\\t5embeds\\test_embeds.npy\n",
      "data\\t5embeds\\test_ids.npy\n",
      "data\\t5embeds\\train_embeds.npy\n",
      "data\\t5embeds\\train_ids.npy\n"
     ]
    }
   ],
   "source": [
    "MAIN_DIR = \"data\"\n",
    "WORK_DIR = \"working\"\n",
    "DATA_DIR = MAIN_DIR + \"/cafa-5-protein-function-prediction\"\n",
    "PROTBERT_DIR = MAIN_DIR + \"/protbert-embeddings-for-cafa5\"\n",
    "ESM2_DIR = MAIN_DIR + \"/cafa-5-esm-2-embeddings-numpy\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(MAIN_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:08:29.084666200Z",
     "start_time": "2024-02-24T19:08:28.941669500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "    ProteinID       GO_ID  Probability\n0  A0A0A0MRZ7  GO:0000001        0.123\n1  A0A0A0MRZ7  GO:0000002        0.123\n2  A0A0A0MRZ8  GO:0000001        0.123\n3  A0A0A0MRZ8  GO:0000002        0.123\n4  A0A0A0MRZ9  GO:0000001        0.123\n5  A0A0A0MRZ9  GO:0000002        0.123\n6  A0A0A0MS00  GO:0000001        0.123\n7  A0A0A0MS00  GO:0000002        0.123\n8  A0A0A0MS01  GO:0000001        0.123\n9  A0A0A0MS01  GO:0000002        0.123",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ProteinID</th>\n      <th>GO_ID</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A0A0MRZ7</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A0A0MRZ7</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A0A0MRZ8</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A0A0MRZ8</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A0A0MRZ9</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A0A0A0MRZ9</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>A0A0A0MS00</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>A0A0A0MS00</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>A0A0A0MS01</td>\n      <td>GO:0000001</td>\n      <td>0.123</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A0A0A0MS01</td>\n      <td>GO:0000002</td>\n      <td>0.123</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a sample submission for Kaggle competition\n",
    "submission = pd.read_csv(f'{DATA_DIR}/sample_submission.tsv', sep='\\t', header=None)\n",
    "submission.columns = [\"ProteinID\", \"GO_ID\", \"Probability\"]\n",
    "submission.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:08:29.312830600Z",
     "start_time": "2024-02-24T19:08:28.983669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda - NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# define important configurations of the code\n",
    "class config:\n",
    "    train_sequences_path = DATA_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = DATA_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = DATA_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "\n",
    "    num_labels = 10000\n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    gamma = 0.7\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device} - {torch.cuda.get_device_name(device)}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:08:29.385830400Z",
     "start_time": "2024-02-24T19:08:29.277668600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# # ______________________ GET PROT BERT EMBEDDINGS WITH HUGGING FACE __________________________________\n",
    "#\n",
    "# # PROT BERT LOADING :\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "#\n",
    "# def get_bert_embedding(\n",
    "#     sequence : str,\n",
    "#     len_seq_limit : int\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "#\n",
    "#     INPUTS:\n",
    "#     - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "#     - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "#\n",
    "#     OUTPUTS:\n",
    "#     - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "#     \"\"\"\n",
    "#     sequence_w_spaces = ' '.join(list(sequence))\n",
    "#     encoded_input = tokenizer(\n",
    "#         sequence_w_spaces,\n",
    "#         truncation=True,\n",
    "#         max_length=len_seq_limit,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt').to(config.device)\n",
    "#     output = model(**encoded_input)\n",
    "#     output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "#     assert len(output_hidden)==1024\n",
    "#     return output_hidden\n",
    "#\n",
    "# ### COLLECTING FOR TRAIN SAMPLES :\n",
    "# print(\"Loading train set ProtBERT Embeddings...\")\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "# fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "#\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint = 0\n",
    "#\n",
    "# for item in tqdm(fasta_train):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#\n",
    "#     if checkpoint>=100:\n",
    "#         df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "#         np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elapsed Time:',time.time()-t0)\n",
    "#\n",
    "# ### COLLECTING FOR TEST SAMPLES :\n",
    "# print(\"Loading test set ProtBERT Embeddings...\")\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "# fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "# ids_list = []\n",
    "# embed_vects_list = []\n",
    "# t0 = time.time()\n",
    "# checkpoint=0\n",
    "# for item in tqdm(fasta_test):\n",
    "#     ids_list.append(item.id)\n",
    "#     embed_vects_list.append(\n",
    "#         get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "#     checkpoint+=1\n",
    "#     if checkpoint>=100:\n",
    "#         np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "#         np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "#         checkpoint=0\n",
    "#\n",
    "# np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "# np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "# print('Total Elasped Time:',time.time()-t0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:08:29.510830600Z",
     "start_time": "2024-02-24T19:08:29.313828900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE TARGETS FOR ENTRY IDS (10000 MOST COMMON GO TERMS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142246it [15:39, 151.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION FINISHED!\n"
     ]
    },
    {
     "data": {
      "text/plain": "      EntryID                                        labels_vect\n0      Q9ZSA8  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n1      P25353  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n2  A0A2R8YCW8  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n3      G3V5N8  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...\n4  A0A140LFN4  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q9ZSA8</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P25353</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A2R8YCW8</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>G3V5N8</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A140LFN4</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### SCRIPT FOR LABELS (TARGETS) VECTORS COLLECTING #####\n",
    "\n",
    "print(f\"GENERATE TARGETS FOR ENTRY IDS ({config.num_labels} MOST COMMON GO TERMS)\")\n",
    "ids = np.load(f\"{ESM2_DIR}/train_ids.npy\")\n",
    "labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "\n",
    "top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "labels_names = top_terms[:config.num_labels].index.values\n",
    "train_labels_sub = labels[(labels.term.isin(labels_names)) & (labels.EntryID.isin(ids))]\n",
    "id_labels = train_labels_sub.groupby('EntryID')['term'].apply(list).to_dict()\n",
    "\n",
    "go_terms_map = {label: i for i, label in enumerate(labels_names)}\n",
    "labels_matrix = np.empty((len(ids), len(labels_names)))\n",
    "\n",
    "for index, id in tqdm(enumerate(ids)):\n",
    "    id_gos_list = id_labels[id]\n",
    "    temp = [go_terms_map[go] for go in labels_names if go in id_gos_list]\n",
    "    labels_matrix[index, temp] = 1\n",
    "\n",
    "labels_list = []\n",
    "for l in range(labels_matrix.shape[0]):\n",
    "    labels_list.append(labels_matrix[l, :])\n",
    "\n",
    "labels_df = pd.DataFrame(data={\"EntryID\":ids, \"labels_vect\":labels_list})\n",
    "labels_df.to_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}.pkl\")\n",
    "print(\"GENERATION FINISHED!\")\n",
    "labels_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T01:30:43.790336500Z",
     "start_time": "2024-02-24T01:14:29.558597100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save and labels as a sparse matrix\n",
    "labels_list_sparse = csr_matrix(labels_list)\n",
    "labels_list_sparse.to_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}_sparse.pkl\")\n",
    "labels_list_sparse = pd.read_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}_sparse.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0000, 0.0000, 0.0255,  ..., 0.0000, 3.6413, 2.1203])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load GO_weights (IA data) as a tensor to feed into the loss function\n",
    "\n",
    "GO_weight_dataset = pd.read_table(f'{DATA_DIR}/IA.txt', header=None, names=['GO', 'weight'])\n",
    "GO_weights = []\n",
    "for each_label in labels_names:\n",
    "    GO_weights.append(GO_weight_dataset.loc[GO_weight_dataset['GO'] == each_label]['weight'].values[0])\n",
    "\n",
    "GO_weights = torch.tensor(GO_weights, dtype=torch.float32)\n",
    "torch.save(GO_weights, f\"{ESM2_DIR}/go_weights_{config.num_labels}.pt\")\n",
    "GO_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T01:32:14.929630900Z",
     "start_time": "2024-02-24T01:32:14.906445600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "           EntryID                                        labels_vect\n0           Q9ZSA8  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n1           P25353  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n2       A0A2R8YCW8  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n3           G3V5N8  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...\n4       A0A140LFN4  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n...            ...                                                ...\n142241      O81299  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...\n142242      Q55AH8  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...\n142243      Q80VK8  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n142244      Q8IS12  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n142245      P0AG74  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...\n\n[142246 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q9ZSA8</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P25353</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A2R8YCW8</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>G3V5N8</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A140LFN4</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>142241</th>\n      <td>O81299</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>142242</th>\n      <td>Q55AH8</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>142243</th>\n      <td>Q80VK8</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>142244</th>\n      <td>Q8IS12</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>142245</th>\n      <td>P0AG74</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>142246 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IF you already have saved the labels and go_weights\n",
    "labels_df = pd.read_pickle(f\"{ESM2_DIR}/train_targets_top{config.num_labels}.pkl\")\n",
    "# GO_weights = torch.load(f\"{ESM2_DIR}/go_weights_{config.num_labels}.pt\")\n",
    "# print(f'Labels shape: {labels_df.shape}, GO Weights shape: {GO_weights.shape}')\n",
    "labels_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:09:11.444975300Z",
     "start_time": "2024-02-24T19:08:29.314831400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors :\n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"ESM2\" : \"cafa-5-esm-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"ESM2\" : 1280\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:09:11.486976500Z",
     "start_time": "2024-02-24T19:09:11.444975300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "      EntryID                                              embed  \\\n0      Q9ZSA8  [-0.092291094, -0.066283956, -0.01226195, 0.04...   \n1      P25353  [0.011624349, -0.030317612, -0.0058019715, 0.0...   \n2  A0A2R8YCW8  [0.02737274, -0.041047025, -0.029205365, 0.028...   \n3      G3V5N8  [0.033766113, -0.07888931, -0.05974137, 0.0456...   \n4  A0A140LFN4  [0.0119482, -0.002107593, -0.084922194, 0.0687...   \n5      B8ZZU6  [0.020136692, -0.13927373, -0.04045331, 0.0626...   \n6      Q01850  [0.026013047, -0.08974387, -0.020240448, 0.132...   \n7      P11076  [0.005006821, -0.03306428, -0.037696116, 0.059...   \n8      Q9VJ64  [0.029283574, -0.010495956, -0.013060905, 0.02...   \n9      Q7YSJ4  [0.02852764, -0.049777817, -0.056415968, 0.141...   \n\n                                         labels_vect  \n0  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n1  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n2  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n3  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n4  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n5  [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...  \n6  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n8  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n9  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EntryID</th>\n      <th>embed</th>\n      <th>labels_vect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q9ZSA8</td>\n      <td>[-0.092291094, -0.066283956, -0.01226195, 0.04...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P25353</td>\n      <td>[0.011624349, -0.030317612, -0.0058019715, 0.0...</td>\n      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A2R8YCW8</td>\n      <td>[0.02737274, -0.041047025, -0.029205365, 0.028...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>G3V5N8</td>\n      <td>[0.033766113, -0.07888931, -0.05974137, 0.0456...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A0A140LFN4</td>\n      <td>[0.0119482, -0.002107593, -0.084922194, 0.0687...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>B8ZZU6</td>\n      <td>[0.020136692, -0.13927373, -0.04045331, 0.0626...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Q01850</td>\n      <td>[0.026013047, -0.08974387, -0.020240448, 0.132...</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>P11076</td>\n      <td>[0.005006821, -0.03306428, -0.037696116, 0.059...</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Q9VJ64</td>\n      <td>[0.029283574, -0.010495956, -0.013060905, 0.02...</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Q7YSJ4</td>\n      <td>[0.02852764, -0.049777817, -0.056415968, 0.141...</td>\n      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset to store embeddings of different sources\n",
    "    It could be used to get training or test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        if embeddings_source in [\"ProtBERT\", \"ESM2\"]:\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(f\"{MAIN_DIR}/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels = pd.read_pickle(\n",
    "                f\"{MAIN_DIR}/{embeds_map[embeddings_source]}/train_targets_top{config.num_labels}.pkl\")\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=\"ESM2\")\n",
    "dataset.df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:10:03.256104Z",
     "start_time": "2024-02-24T19:09:11.445975800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPONENTS FOR FIRST PROTEIN:  \n",
      "EMBEDDINGS VECTOR: \n",
      "  tensor([-0.0923, -0.0663, -0.0123,  ..., -0.1607,  0.0159,  0.0017]) \n",
      "\n",
      "TARGETS LABELS VECTOR: \n",
      "  tensor([0., 1., 0.,  ..., 0., 0., 0.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = dataset.__getitem__(0)\n",
    "print(\"COMPONENTS FOR FIRST PROTEIN:  \")\n",
    "print(\"EMBEDDINGS VECTOR: \\n \", embeddings, \"\\n\")\n",
    "print(\"TARGETS LABELS VECTOR: \\n \", labels, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:10:03.445850500Z",
     "start_time": "2024-02-24T19:10:03.255105100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep Residual Network module with step by step predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, step_dim):\n",
    "        super(ResidualNetwork, self).__init__()\n",
    "\n",
    "        self.forward_linear1 = torch.nn.Linear(input_dim, input_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear1 = torch.nn.Linear(input_dim, step_dim[0])\n",
    "        self.sigmoid1 = torch.nn.Sigmoid()\n",
    "\n",
    "        self.forward_linear2 = torch.nn.Linear(input_dim + step_dim[0], input_dim + step_dim[0])\n",
    "        self.batchnorm2 = nn.BatchNorm1d(input_dim + step_dim[0])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear2 = torch.nn.Linear(input_dim + step_dim[0], step_dim[1])\n",
    "        self.sigmoid2 = torch.nn.Sigmoid()\n",
    "\n",
    "        self.forward_linear3 = torch.nn.Linear(input_dim + step_dim[0] + step_dim[1], input_dim + step_dim[0] + step_dim[1])\n",
    "        self.batchnorm3 = nn.BatchNorm1d(input_dim + step_dim[0] + step_dim[1])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear3 = torch.nn.Linear(input_dim + step_dim[0] + step_dim[1], step_dim[2])\n",
    "        self.sigmoid3 = torch.nn.Sigmoid()\n",
    "\n",
    "        self.forward_linear4 = torch.nn.Linear(input_dim + step_dim[0] + step_dim[1] + step_dim[2], input_dim + step_dim[0] + step_dim[1] + step_dim[2])\n",
    "        self.batchnorm4 = nn.BatchNorm1d(input_dim + step_dim[0] + step_dim[1] + step_dim[2])\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout()\n",
    "\n",
    "        self.pred_linear4 = torch.nn.Linear(input_dim + step_dim[0] + step_dim[1] + step_dim[2], step_dim[3])\n",
    "        self.sigmoid4 = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation11(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        y1 = self.sigmoid1(self.pred_linear1(x))\n",
    "\n",
    "        x = torch.cat([x, y1], dim=1)\n",
    "        x = self.forward_linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        y2 = self.sigmoid2(self.pred_linear2(x))\n",
    "\n",
    "        x = torch.cat([x, y2], dim=1)\n",
    "        x = self.forward_linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        y3 = self.sigmoid3(self.pred_linear3(x))\n",
    "\n",
    "        x = torch.cat([x, y3], dim=1)\n",
    "        x = self.forward_linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        y4 = self.sigmoid4(self.pred_linear4(x))\n",
    "\n",
    "        y = torch.cat([y1, y2, y3, y4], dim=1)\n",
    "\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:14:54.122332400Z",
     "start_time": "2024-02-24T19:14:54.099585800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Adjusted MLP model with 6 linear layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 1280)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(1280)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(1280, 1800)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1800)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(1800, 2560)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.batchnorm3 = nn.BatchNorm1d(2560)\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.linear4 = torch.nn.Linear(2560, 3200)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.batchnorm4 = nn.BatchNorm1d(3200)\n",
    "        self.dropout4 = nn.Dropout()\n",
    "\n",
    "        self.linear5 = torch.nn.Linear(3200, 4200)\n",
    "        self.activation5 = torch.nn.ReLU()\n",
    "        self.batchnorm5 = nn.BatchNorm1d(4200)\n",
    "        self.dropout5 = nn.Dropout()\n",
    "\n",
    "        self.linear6 = torch.nn.Linear(4200, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.linear5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = self.linear6(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T01:37:05.487640500Z",
     "start_time": "2024-02-24T01:37:05.478642600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN-1D model to make predictions using CLS token embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=1024)       # 1024 is better\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)                # 1024 is better\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T01:37:05.871945700Z",
     "start_time": "2024-02-24T01:37:05.852514300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_train_val_dataloader(embeddings_source, train_size=0.9):\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source=embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset,\n",
    "                                      lengths=[int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:14:56.263336200Z",
     "start_time": "2024-02-24T19:14:56.251228800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, embeddings_source, model_type):\n",
    "    \"\"\"\n",
    "    Custom function to train the baseline model on dataset\n",
    "    :param val_dataloader: dataloader for validation data\n",
    "    :param train_dataloader: dataloader for training data\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    :param model_type: define the type of model\n",
    "    \"\"\"\n",
    "    if model_type == 'residual':\n",
    "        model = ResidualNetwork(input_dim=embeds_dim[embeddings_source], step_dim=[800, 1200, 1400, 1600])\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    if model_type == \"conv\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels)\n",
    "\n",
    "    print(f'Model:\\n{model}')\n",
    "    model.to(config.device)\n",
    "\n",
    "    # define configurations of the model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=config.gamma)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "\n",
    "    # multilabel prediction task, GO_weights could be used as weight in the loss function and f1score\n",
    "    GO_weights.to(config.device)\n",
    "    MultiLabelLoss = torch.nn.BCEWithLogitsLoss(weight=GO_weights+1)\n",
    "    # f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_f1score_history, val_f1score_history = [], []\n",
    "    train_accuracy_history, val_accuracy_history = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "\n",
    "        ## TRAIN PHASE :\n",
    "        model.train()\n",
    "        losses, scores, accuracy = [], [], []\n",
    "\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            preds = (sigmoid(preds).detach().cpu().numpy() > 0.5).astype(int)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            score = f1_score(targets, preds, average='weighted', sample_weight=GO_weights)\n",
    "            acc = np.mean([accuracy_score(targets[i], preds[i], sample_weight=GO_weights) for i in range(len(targets))])\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score)\n",
    "            accuracy.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        avg_accuracy = np.mean(accuracy)\n",
    "        print(\"Running Average TRAIN Loss: \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score: \", avg_score)\n",
    "        print(\"Running Average TRAIN Accuracy: \", avg_accuracy)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "        train_accuracy_history.append(avg_accuracy)\n",
    "\n",
    "        ## VALIDATION PHASE :\n",
    "        model.eval()\n",
    "        losses, scores, accuracy = [], [], []\n",
    "\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "\n",
    "            loss = MultiLabelLoss(preds, targets)\n",
    "            preds = (sigmoid(preds).detach().cpu().numpy() > 0.5).astype(int)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            score = f1_score(targets, preds, average='weighted', sample_weight=GO_weights)\n",
    "            acc = np.mean([accuracy_score(targets[i], preds[i], sample_weight=GO_weights) for i in range(len(targets))])\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score)\n",
    "            accuracy.append(acc)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        avg_accuracy = np.mean(accuracy)\n",
    "        print(\"Running Average VAL Loss: \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score: \", avg_score)\n",
    "        print(\"Running Average VAL Accuracy: \", avg_accuracy)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "        val_accuracy_history.append(avg_accuracy)\n",
    "\n",
    "        scheduler.step(), print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"TRAINING FINISHED _____\")\n",
    "    print(f\"FINAL TRAINING F1SCORE: {train_f1score_history[-1]},  ACCURACY: {train_accuracy_history[-1]}\")\n",
    "    print(f\"FINAL VALIDATION F1SCORE: {val_f1score_history[-1]},  ACCURACY: {val_accuracy_history[-1]}\")\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "    accuracy_history = {\"train\" : train_accuracy_history, \"val\" : val_accuracy_history}\n",
    "\n",
    "    return model, losses_history, scores_history, accuracy_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T19:15:14.382443600Z",
     "start_time": "2024-02-24T19:15:14.375818800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_train_val_dataloader(\"ESM2\")\n",
    "\n",
    "esm2_model, esm2_losses, esm2_scores, esm2_accuracy = train_model(train_dataloader,\n",
    "                                                                  val_dataloader,\n",
    "                                                                  embeddings_source=\"ESM2\",\n",
    "                                                                  model_type=\"residual\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-24T19:01:13.647956200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = next(iter(val_dataloader))[0][1].unsqueeze(0)\n",
    "label = next(iter(val_dataloader))[1][1].unsqueeze(0)\n",
    "print(f\"Sample:\\n{sample}\\n\\nLabel:\\n{label}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model.to('cpu')\n",
    "esm2_model.eval()\n",
    "output = esm2_model(sample)\n",
    "output = torch.round(sigmoid(output))\n",
    "output = output.detach().cpu().numpy()\n",
    "accuracy_score(label[0], output[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t33_650M_UR50D')\n",
    "embedding_model = EsmModel.from_pretrained('facebook/esm2_t33_650M_UR50D', add_cross_attention=False, is_decoder=False)\n",
    "embedding_model.eval()\n",
    "embedding_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ids = tokenizer(['MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN', 'QQQQQ'], add_special_tokens=True, padding=\"longest\")\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])\n",
    "output = embedding_model(input_ids=input_ids, attention_mask=attention_mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = esm2_model(output.last_hidden_state[0].to(device))\n",
    "cls = esm2_model(output.last_hidden_state[0][0].to(device))\n",
    "torch.nn.functional.sigmoid(cls) > 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "esm2_model(dataset[0][0].reshape(1, -1).to(config.device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(embeddings_source, data_source):\n",
    "    \"\"\"\n",
    "    Custom function to make inference using the model\n",
    "    :param embeddings_source: define the type of embedding\n",
    "    \"\"\"\n",
    "\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "\n",
    "    if data_source == \"val\":\n",
    "        predict_dataloader = val_dataloader\n",
    "\n",
    "    if data_source == 'test':\n",
    "        predict_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"ESM2\":\n",
    "        model = esm2_model\n",
    "\n",
    "    # Set model on evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(predict_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(predict_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_df = predict(\"T5\", \"val\")\n",
    "submission_df.to_tsv(\"/working/predictions_val.tsv\")\n",
    "submission_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### SCRIPT TO EVALUATE PREDICTIONS USING CAFA EVALUATOR ###\n",
    "\n",
    "import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval\n",
    "\n",
    "cafa_eval(f\"{DATA_DIR}/Train/go-basic.obo\", submission_df, f\"{DATA_DIR}/Train/train_terms.tsv\", ia=f\"{DATA_DIR}/IA.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### IN PROGRESS - SCRIPT TO TRAIN THE MODEL USING PyTorchLightning ###\n",
    "\n",
    "class Linear_Lightning(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    In progress, used to train the MLP model on multiple GPUs using PyTorchLightning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, train_size, **hparams):\n",
    "        super(Linear_Lightning, self).__init__()\n",
    "\n",
    "        self.model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "        train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "        self.train_set, self.val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n",
    "        self.accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        logs = {\"train_loss\" : loss, \"f1_score\" : f1_score, \"accuracy_score\" : acc_score}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        embed, targets = batch\n",
    "        preds = self(embed)\n",
    "        loss= self.loss_fn(preds, targets)\n",
    "        f1_score = self.f1_score(preds, targets)\n",
    "        acc_score = self.accuracy(preds, targets)\n",
    "\n",
    "        return {\"val_loss\": loss, \"f1_score\": f1_score, \"accuracy_score\": acc_score}\n",
    "\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in ouputs]).mean()\n",
    "        logs = {\"val_loss\" : avg_loss}\n",
    "        self.log_dict(\n",
    "            logs,\n",
    "            on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": logs}\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataloader = torch.utils.data.DataLoader(self.val_set, batch_size=config.batch_size, shuffle=False,)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataloader = torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.n_epochs,\n",
    "    limit_train_batches=5000,\n",
    "    logger=logger)\n",
    "\n",
    "\n",
    "model = Linear_Lightning(\n",
    "    input_dim=embeds_dim[embeddings_source],\n",
    "    num_classes=config.num_labels,\n",
    "    train_size=0.8)\n",
    "\n",
    "\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
